{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "[97, 103, 99, 116]\n"
     ]
    }
   ],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "list_of_lists = []\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        list_of_lists.append(inner_list)\n",
    "print(len(list_of_lists[0])) # About 4 MB\n",
    "ecoli = list_of_lists[0]\n",
    "\n",
    "n_symb = 4\n",
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "k = 64 #context length\n",
    "n = 100000 # train every\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# #Train model\n",
    "# x = np.zeros((update_period-k, k, vocab_size, 1)) # 64 characters context\n",
    "# y = np.zeros((update_period-k, vocab_size))\n",
    "\n",
    "\n",
    "# print(len(s))\n",
    "# idx3 = 0\n",
    "# for idx2 in range(k,len(s)):\n",
    "#     train_seq = [to_categorical(legend[i], num_classes=vocab_size ) for i in s[idx2-k:idx2]] \n",
    "#     train_target = to_categorical(legend[s[idx2]], num_classes=vocab_size )\n",
    "#     x[idx3,:] = np.array(train_seq).reshape((k, vocab_size, 1))\n",
    "#     y[idx3,:] = train_target\n",
    "#     idx3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = 0\n",
    "n_symb = 4\n",
    "x = np.zeros((200000, k, n_symb, 1)) # 64 characters context\n",
    "y = np.zeros((200000, n_symb))\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(200000*overall+k,200000*(overall+1)+k): #len(s)):\n",
    "    train_seq = [to_categorical(legend[i], num_classes=n_symb ) for i in s[idx2-k:idx2]] \n",
    "    train_target = to_categorical(legend[s[idx2]], num_classes=n_symb )\n",
    "    x[idx3,:] = np.array(train_seq).reshape(k, n_symb, 1)\n",
    "    y[idx3] = train_target\n",
    "    idx3 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sounds cool but my computer can't handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 256  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 352  #Size of the LSTM layer\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "cnn_lstm = Sequential()\n",
    "cnn_lstm.add(Conv2D(1, (2,2), activation='relu', padding='same', input_shape=(k, vocab_size,1)))\n",
    "cnn_lstm.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "cnn_lstm.add(LSTM(lstm_cells, return_sequences=True))\n",
    "cnn_lstm.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "cnn_lstm.add(LSTM(lstm_cells, return_sequences=True))\n",
    "cnn_lstm.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "cnn_lstm.add(LSTM(lstm_cells, return_sequences=True))\n",
    "cnn_lstm.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "cnn_lstm.add(LSTM(lstm_cells))\n",
    "cnn_lstm.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "cnn_lstm.add(Dense(vocab_size))\n",
    "cnn_lstm.add(Activation('softmax'))\n",
    "cnn_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# cnn_lstm.fit(x[:100000],y[:100000],\n",
    "#                   batch_size=250,\n",
    "#                   epochs=1,\n",
    "#                   validation_data=(x[100000:],y[100000:]))\n",
    "# cnn_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//n - 1):\n",
    "    if i%5 == 0:\n",
    "        print(i)\n",
    "    cnn_lstm.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)], batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(clf.predict_proba(x[n*(i+1):n*(i+2)]))\n",
    "predicted_onehot += list(clf.predict_proba(x[n*(len(ecoli)//n):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ecoli)//200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ecoli)//500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 54s 540us/sample - loss: 1.4246 - categorical_accuracy: 0.2678 - val_loss: 1.3850 - val_categorical_accuracy: 0.2529\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 39s 386us/sample - loss: 1.3839 - categorical_accuracy: 0.2828 - val_loss: 1.3778 - val_categorical_accuracy: 0.2927\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 29s 289us/sample - loss: 1.3817 - categorical_accuracy: 0.2856\n",
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 38s 384us/sample - loss: 1.3825 - categorical_accuracy: 0.2820 - val_loss: 1.3769 - val_categorical_accuracy: 0.2880\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 39s 385us/sample - loss: 1.3817 - categorical_accuracy: 0.2840 - val_loss: 1.3849 - val_categorical_accuracy: 0.2709\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 29s 289us/sample - loss: 1.3771 - categorical_accuracy: 0.2952\n",
      "4638690\n"
     ]
    }
   ],
   "source": [
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_CNNLSTM.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(24):\n",
    "        predicted_val = []\n",
    "        if overall <= 22:\n",
    "            x = np.zeros((200000, k, n_symb, 1)) # 64 characters context\n",
    "            y = np.zeros((200000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,200000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [to_categorical(legend[i], num_classes=n_symb ) for i in s[idx2-k:idx2]]  \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(k, n_symb, 1)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 23:\n",
    "            x = np.zeros((len(s)-200000*overall-k, k, n_symb, 1)) # 64 characters context\n",
    "            y = np.zeros((len(s)-200000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [to_categorical(legend[i], num_classes=n_symb ) for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(k, n_symb, 1)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(cnn_lstm.predict(x[0:n]))\n",
    "            \n",
    "\n",
    "        for i in range(1):\n",
    "            cnn_lstm.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "\n",
    "            predicted_val += list(cnn_lstm.predict(x[n*(i+1):n*(i+2)]))\n",
    "#         i = len(x)//100000 -2\n",
    "#         cnn_lstm.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "#                   batch_size=250,\n",
    "#                   epochs=1,\n",
    "#                   validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "#         predicted_val += list(cnn_lstm.predict(x[n*(i+1):]))\n",
    "\n",
    "        cnn_lstm.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=1)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    \n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
