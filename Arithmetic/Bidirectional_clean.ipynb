{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "from plist import ProbabilityList\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress and Decompress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_18 (Bidirectio (None, 10, 64)            9984      \n",
      "_________________________________________________________________\n",
      "layer_normalization_18 (Laye (None, 10, 64)            20        \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "layer_normalization_19 (Laye (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 35,224\n",
      "Trainable params: 35,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "with open('data/ecoli/Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        ecoli = list(line)\n",
    "\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "s =  [legend[temp_dict[i]] for i in ecoli]\n",
    "\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 10 #time steps\n",
    "seg_len = 6 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 4\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 32\n",
    "batch_size = 250\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "n_symb = 4\n",
    "\n",
    "BILSTM = Sequential()\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,seg_len), return_sequences=True), input_shape=(tsteps,seg_len)))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size))))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Dense(n_symb))\n",
    "BILSTM.add(Activation('softmax'))\n",
    "BILSTM.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "BILSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 19s 187us/sample - loss: 1.4005 - categorical_accuracy: 0.2886 - val_loss: 1.3704 - val_categorical_accuracy: 0.3052\n",
      "[[0.10682002 0.23681933 0.32078034 0.33558035]]\n",
      "[0.10682004 0.23681933 0.3207804  0.3355803 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3724 - categorical_accuracy: 0.3021\n",
      "200059\n",
      "200059\n",
      "1\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3703 - categorical_accuracy: 0.3051 - val_loss: 1.3609 - val_categorical_accuracy: 0.3197\n",
      "[[0.16085665 0.21093686 0.3661135  0.26209292]]\n",
      "[0.16085666 0.21093689 0.36611354 0.26209292]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3623 - categorical_accuracy: 0.3197\n",
      "400059\n",
      "400059\n",
      "2\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3504 - categorical_accuracy: 0.3340 - val_loss: 1.3658 - val_categorical_accuracy: 0.3197\n",
      "[[0.22976562 0.45477754 0.15848337 0.15697353]]\n",
      "[0.22976558 0.45477754 0.15848337 0.15697348]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3506 - categorical_accuracy: 0.3344\n",
      "600059\n",
      "600059\n",
      "3\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3482 - categorical_accuracy: 0.3367 - val_loss: 1.3461 - val_categorical_accuracy: 0.3348\n",
      "[[0.34205866 0.21478042 0.19060406 0.25255686]]\n",
      "[0.34205866 0.21478042 0.19060406 0.25255683]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3440 - categorical_accuracy: 0.3398\n",
      "800059\n",
      "800059\n",
      "4\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3395 - categorical_accuracy: 0.3465 - val_loss: 1.3445 - val_categorical_accuracy: 0.3427\n",
      "[[0.28499782 0.26763287 0.27802828 0.16934104]]\n",
      "[0.28499782 0.26763287 0.27802828 0.16934103]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3415 - categorical_accuracy: 0.3415\n",
      "1000059\n",
      "1000059\n",
      "5\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3450 - categorical_accuracy: 0.3387 - val_loss: 1.3432 - val_categorical_accuracy: 0.3401\n",
      "[[0.25097355 0.23550521 0.15837172 0.35514948]]\n",
      "[0.2509736  0.23550521 0.15837172 0.35514948]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3398 - categorical_accuracy: 0.3437\n",
      "1200059\n",
      "1200059\n",
      "6\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3472 - categorical_accuracy: 0.3354 - val_loss: 1.3397 - val_categorical_accuracy: 0.3465\n",
      "[[0.16250196 0.13320644 0.37157688 0.33271477]]\n",
      "[0.16250195 0.13320644 0.37157694 0.33271474]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 72us/sample - loss: 1.3399 - categorical_accuracy: 0.3455\n",
      "1400059\n",
      "1400059\n",
      "7\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3437 - categorical_accuracy: 0.3401 - val_loss: 1.3518 - val_categorical_accuracy: 0.3320\n",
      "[[0.142209  0.3581357 0.1361492 0.3635061]]\n",
      "[0.14220902 0.35813573 0.13614915 0.36350608]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3469 - categorical_accuracy: 0.3378\n",
      "1600059\n",
      "1600059\n",
      "8\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3454 - categorical_accuracy: 0.3400 - val_loss: 1.3406 - val_categorical_accuracy: 0.3425\n",
      "[[0.2984873  0.19177109 0.21848066 0.29126093]]\n",
      "[0.29848734 0.19177106 0.21848069 0.29126096]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3400 - categorical_accuracy: 0.3446\n",
      "1800059\n",
      "1800059\n",
      "9\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3391 - categorical_accuracy: 0.3466 - val_loss: 1.3365 - val_categorical_accuracy: 0.3492\n",
      "[[0.3352718  0.12610734 0.3108131  0.2278078 ]]\n",
      "[0.3352718  0.1261073  0.31081307 0.2278078 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3358 - categorical_accuracy: 0.3505\n",
      "2000059\n",
      "2000059\n",
      "10\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3412 - categorical_accuracy: 0.3419 - val_loss: 1.3363 - val_categorical_accuracy: 0.3517\n",
      "[[0.21322675 0.17990552 0.20059533 0.40627244]]\n",
      "[0.21322675 0.17990552 0.20059533 0.40627244]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3323 - categorical_accuracy: 0.3539\n",
      "2200059\n",
      "2200059\n",
      "11\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3310 - categorical_accuracy: 0.3565 - val_loss: 1.3316 - val_categorical_accuracy: 0.3551\n",
      "[[0.30734414 0.23319876 0.22618341 0.23327366]]\n",
      "[0.30734414 0.23319875 0.22618341 0.23327366]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3311 - categorical_accuracy: 0.3551\n",
      "2400059\n",
      "2400059\n",
      "12\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3405 - categorical_accuracy: 0.3458 - val_loss: 1.3327 - val_categorical_accuracy: 0.3537\n",
      "[[0.16882233 0.29516506 0.3181532  0.21785946]]\n",
      "[0.16882232 0.29516503 0.3181532  0.21785943]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3288 - categorical_accuracy: 0.3578\n",
      "2600059\n",
      "2600059\n",
      "13\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3233 - categorical_accuracy: 0.3628 - val_loss: 1.3614 - val_categorical_accuracy: 0.3250\n",
      "[[0.23680843 0.3197375  0.24387547 0.19957855]]\n",
      "[0.23680846 0.31973752 0.24387547 0.19957855]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3469 - categorical_accuracy: 0.3377\n",
      "2800059\n",
      "2800059\n",
      "14\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3279 - categorical_accuracy: 0.3578 - val_loss: 1.3386 - val_categorical_accuracy: 0.3473\n",
      "[[0.22505125 0.46360007 0.14570253 0.16564608]]\n",
      "[0.22505131 0.46360007 0.14570251 0.16564606]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3364 - categorical_accuracy: 0.3486\n",
      "3000059\n",
      "3000059\n",
      "15\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3292 - categorical_accuracy: 0.3547 - val_loss: 1.3343 - val_categorical_accuracy: 0.3484\n",
      "[[0.24795687 0.29482338 0.202236   0.25498375]]\n",
      "[0.24795686 0.2948234  0.20223603 0.25498375]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3328 - categorical_accuracy: 0.3507\n",
      "3200059\n",
      "3200059\n",
      "16\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3280 - categorical_accuracy: 0.3582 - val_loss: 1.3305 - val_categorical_accuracy: 0.3547\n",
      "[[0.35650343 0.24039528 0.22585429 0.17724705]]\n",
      "[0.35650334 0.2403953  0.2258543  0.17724703]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3266 - categorical_accuracy: 0.3574\n",
      "3400059\n",
      "3400059\n",
      "17\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3373 - categorical_accuracy: 0.3443 - val_loss: 1.3290 - val_categorical_accuracy: 0.3569\n",
      "[[0.12689638 0.23692253 0.3957916  0.24038951]]\n",
      "[0.12689634 0.2369225  0.39579165 0.24038947]\n",
      "Train on 100000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3235 - categorical_accuracy: 0.3627\n",
      "3600059\n",
      "3600059\n",
      "18\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3263 - categorical_accuracy: 0.3598 - val_loss: 1.3339 - val_categorical_accuracy: 0.3513\n",
      "[[0.34034675 0.20532262 0.20383005 0.25050053]]\n",
      "[0.34034678 0.20532264 0.20383006 0.25050053]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3316 - categorical_accuracy: 0.3535\n",
      "3800059\n",
      "3800059\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "\n",
    "# from tensorflow import random\n",
    "# random.set_seed(1)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "inputfile, outputfile = 'data/ecoli/Ecoli.txt', 'data/ecoli/Ecoli.bi_simple_seed1'\n",
    "epochs = 1\n",
    "e_idx = 0\n",
    "\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    \n",
    "    \n",
    "    ## For the first n+k characters, we compress with default method\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol)\n",
    "        h = model.get_high(symbol)\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(symbol)\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [0 for i in range(257)]\n",
    "    prior[:4] = [0.25,0.25,0.25,0.25]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model, now e_idx = n+k\n",
    "    \n",
    "    for overall in range(len(s)//200000 + 1):\n",
    "        predicted_val = []\n",
    "        if overall < len(s)//200000:\n",
    "            x = np.zeros((200000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((200000, n_symb))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,200000*(overall+1)+k): #len(s)):\n",
    "                train_seq = s[idx2-k:idx2]\n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "                \n",
    "        if overall == len(s)//200000:\n",
    "            x = np.zeros((len(s)-200000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-200000*overall-k, n_symb))\n",
    "            print(len(x))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = s[idx2-k:idx2]\n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0 and overall != len(s)//200000:\n",
    "            predicted_val += list(BILSTM.predict(x[0:n]))\n",
    "        if overall != len(s)//200000:\n",
    "            BILSTM.fit(x[0:n], y[0:n],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x[n:2*n], y[n:2*n]))\n",
    "            \n",
    "            predicted_val += list(BILSTM.predict(x[n:2*n]))\n",
    "            \n",
    "            # For checking\n",
    "            x_arr = np.array(s[200000*(overall+1)-1:200000*(overall+1)+k-1]).reshape(1,tsteps,seg_len)\n",
    "            print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "            print(predicted_val[-1])\n",
    "            \n",
    "            BILSTM.fit(x[n:2*n], y[n:2*n],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs)\n",
    "            \n",
    "        if overall == len(s)//200000:\n",
    "            predicted_val += list(BILSTM.predict(x[:]))\n",
    "            \n",
    "        for prob_list in predicted_val:\n",
    "#             for val, prob in enumerate(prob_list):\n",
    "#                 model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "            model.prob_list[:4] = prob_list\n",
    "            #model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "            model.normalize()\n",
    "            t = int(100000) ## New lines!\n",
    "            l = int(model.get_low(s[e_idx])*100000)\n",
    "            h = int(model.get_high(s[e_idx])*100000)\n",
    "            enc.storeRegion(l,h,t) \n",
    "#             t = model.get_total()\n",
    "#             l = model.get_low(legend[s[e_idx]])\n",
    "#             h = model.get_high(legend[s[e_idx]])\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        if overall != len(s)//200000: ## checking to confirm\n",
    "            print(e_idx-1) \n",
    "            print(200000*(overall+1)+k-1)\n",
    "            \n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "    e_idx += 1\n",
    "    print(e_idx)\n",
    "    enc.finish_encode(bitout)\n",
    "    \n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 10, 64)            9984      \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 10, 64)            20        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 35,224\n",
      "Trainable params: 35,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "with open('data/ecoli/Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        ecoli = list(line)\n",
    "\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "s =  [legend[temp_dict[i]] for i in ecoli]\n",
    "\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 10 #time steps\n",
    "seg_len = 6 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 4\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 32\n",
    "batch_size = 250\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "n_symb = 4\n",
    "\n",
    "BILSTM = Sequential()\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,seg_len), return_sequences=True), input_shape=(tsteps,seg_len)))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size))))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Dense(n_symb))\n",
    "BILSTM.add(Activation('softmax'))\n",
    "BILSTM.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "BILSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 14s 139us/sample - loss: 1.4005 - categorical_accuracy: 0.2886\n",
      "True\n",
      "[[0.24172667 0.20348574 0.26649722 0.28829032]]\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "180000\n",
      "200000\n",
      "[[0.10682002 0.23681933 0.32078034 0.33558035]]\n",
      "200059\n",
      "200059\n",
      "1\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3724 - categorical_accuracy: 0.3021\n",
      "True\n",
      "[[0.09809307 0.24810064 0.22409134 0.42971504]]\n",
      "220000\n",
      "240000\n",
      "260000\n",
      "280000\n",
      "300000\n",
      "[[0.18385561 0.26258686 0.1933979  0.36015964]]\n",
      "300059\n",
      "400059\n",
      "2\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3703 - categorical_accuracy: 0.3051\n",
      "True\n",
      "[[0.1783294  0.27792236 0.22681716 0.3169311 ]]\n",
      "320000\n",
      "340000\n",
      "360000\n",
      "380000\n",
      "400000\n",
      "[[0.16085665 0.21093686 0.3661135  0.26209292]]\n",
      "400059\n",
      "600059\n",
      "3\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 72us/sample - loss: 1.3623 - categorical_accuracy: 0.3197\n",
      "True\n",
      "[[0.1353031  0.31263143 0.32676032 0.22530521]]\n",
      "420000\n",
      "440000\n",
      "460000\n",
      "480000\n",
      "500000\n",
      "[[0.21483691 0.30914095 0.30469945 0.17132269]]\n",
      "500059\n",
      "800059\n",
      "4\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3504 - categorical_accuracy: 0.3340\n",
      "True\n",
      "[[0.17608535 0.38199475 0.31591347 0.12600638]]\n",
      "520000\n",
      "540000\n",
      "560000\n",
      "580000\n",
      "600000\n",
      "[[0.22976562 0.45477754 0.15848337 0.15697353]]\n",
      "600059\n",
      "1000059\n",
      "5\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3506 - categorical_accuracy: 0.3344\n",
      "True\n",
      "[[0.2281469  0.41619614 0.16694225 0.18871474]]\n",
      "620000\n",
      "640000\n",
      "660000\n",
      "680000\n",
      "700000\n",
      "[[0.30334225 0.28820693 0.15581484 0.252636  ]]\n",
      "700059\n",
      "1200059\n",
      "6\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3482 - categorical_accuracy: 0.3367\n",
      "True\n",
      "[[0.32498038 0.25508386 0.16890945 0.25102627]]\n",
      "720000\n",
      "740000\n",
      "760000\n"
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "inputfile, outputfile = 'data/ecoli/Ecoli.bi_simple_seed1', 'data/ecoli/Ecoli_decompressed.txt'\n",
    "epochs = 1\n",
    "e_idx = 0\n",
    "\n",
    "# Perform file decompression\n",
    "with open(inputfile, \"rb\") as inp, open(outputfile, \"wb\") as out:\n",
    "    bitin = arith.BitInputStream(inp)\n",
    "    \n",
    "    \n",
    "    ## For the first n+k characters, we compress with default method\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "    dec = arith.ArithmeticCoder(32)\n",
    "    dec.start_decode(bitin)\n",
    "    new_s = []\n",
    "    while e_idx < n+k:\n",
    "        total = model.get_total()\n",
    "        Range = dec.R\n",
    "        offset = dec.getTarget()\n",
    "        value = dec.getTarget(total)\n",
    "        start = 0\n",
    "        end = model.get_symbol_limit()\n",
    "        while end - start > 1:\n",
    "            middle = (start + end) >> 1\n",
    "            if model.get_low(middle) > value:\n",
    "                end = middle\n",
    "            else:\n",
    "                start = middle\n",
    "        symbol = start\n",
    "        l = model.get_low(symbol) \n",
    "        h = model.get_high(symbol)\n",
    "        dec.loadRegion(l,h,total)\n",
    "        \n",
    "        model.increment(symbol)\n",
    "        out.write(bytes((char_list[symbol],)))\n",
    "        new_s.append(symbol)\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [0 for i in range(257)]\n",
    "    prior[:4] = [0.25,0.25,0.25,0.25]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model, now e_idx = n+k\n",
    "    \n",
    "    for overall in range(len(s)//100000 + 1): # assume we save len(s), this only takes 8 bits, and cut the need for 256\n",
    "\n",
    "        if overall < len(s)//100000:\n",
    "            x = np.zeros((100000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((100000, n_symb))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(100000*overall+k,100000*(overall+1)+k): #len(s)):\n",
    "                train_seq = new_s[idx2-k:idx2]\n",
    "                train_target = new_s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "            BILSTM.fit(x[0:n], y[0:n],\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs) \n",
    "        if overall == len(s)//100000:\n",
    "            segment_len = len(s)-100000*overall-k\n",
    "        else:\n",
    "            segment_len = 100000\n",
    "        print(new_s == s[:len(new_s)])  \n",
    "        temp_x = new_s[-1*k-1:-1]\n",
    "        x_arr = np.array(temp_x).reshape(1,tsteps,seg_len)\n",
    "        print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "        temp_x = new_s[-1*k:]\n",
    "        for i in range(segment_len):\n",
    "            x_arr = np.array(temp_x).reshape(1,tsteps,seg_len)\n",
    "            prob_list_temp = BILSTM(x_arr.astype(np.float32), training= False).numpy()\n",
    "            model.prob_list[:4] = prob_list_temp[0]\n",
    "\n",
    "            model.normalize()\n",
    "            \n",
    "#             print(model.prob_list[:4])\n",
    "#             print(dec.R)\n",
    "#             print(dec.getTarget(total))\n",
    "#             print(model.get_symbol_limit())\n",
    "            total = int(100000) ## New lines!\n",
    "            Range = dec.R\n",
    "            offset = dec.getTarget()\n",
    "            value = dec.getTarget(total)\n",
    "            start = 0\n",
    "            end = model.get_symbol_limit()\n",
    "            while end - start > 1:\n",
    "                middle = (start + end) >> 1\n",
    "                if int(model.get_low(middle)*100000) > value:\n",
    "                    #print(int(model.get_low(middle)*100000))\n",
    "                    end = middle\n",
    "                else:\n",
    "                    start = middle\n",
    "\n",
    "            symbol = start\n",
    "            assert symbol != 256\n",
    "            out.write(bytes((char_list[symbol],)))\n",
    "            \n",
    "            l = int(model.get_low(symbol)*100000)\n",
    "            h = int(model.get_high(symbol)*100000)\n",
    "            dec.loadRegion(l,h,total) \n",
    "\n",
    "            temp_x = temp_x[1:] + [symbol]\n",
    "            new_s.append(symbol)\n",
    "            if e_idx%20000 == 0:\n",
    "                print(e_idx)\n",
    "            e_idx += 1\n",
    "            \n",
    "        \n",
    "        print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "        print(e_idx-1) \n",
    "        print(200000*(overall+1)+k-1)\n",
    "            \n",
    "\n",
    "    e_idx += 1\n",
    "    print(e_idx)\n",
    "    \n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hmmmm', new_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 2, 3, 3, 0, 0, 2, 2, 0, 0, 3, 0, 0, 0, 3, 1, 1, 2, 3, 1, 1, 2, 1, 0, 0, 1]\n",
      "[1, 2, 1, 1, 1, 2, 3, 3, 0, 0, 2, 2, 0, 0, 3, 0, 0, 0, 3, 1, 1, 2, 3, 1, 1, 2, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_s[100030:100060])\n",
    "print(s[100030:100060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 1, 0, 3, 1, 0, 3, 1, 2, 3, 1, 3, 0, 1, 1, 2, 1, 1, 3, 0, 0, 2, 1, 2, 2, 0, 3, 1]\n",
      "[3, 2, 1, 1, 0, 3, 1, 0, 3, 1, 2, 3, 1, 3, 0, 1, 1, 2, 1, 1, 3, 0, 0, 2, 1, 2, 2, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_s[200030:200060])\n",
    "print(s[200030:200060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filecmp.cmp('data/ecoli/Ecoli.txt', 'data/ecoli/Ecoli_decompressed.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
