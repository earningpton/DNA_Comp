{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "[97, 103, 99, 116]\n"
     ]
    }
   ],
   "source": [
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        ecoli = inner_list\n",
    "print(len(ecoli)) # About 4 MB\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s =  [temp_dict[i] for i in ecoli]\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 20 #time steps\n",
    "seg_len = 20 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Embedding\n",
    "# max_features = 20000\n",
    "# maxlen = k\n",
    "# embedding_size = 128\n",
    "\n",
    "# # Convolution\n",
    "# kernel_size = 5\n",
    "# filters = 64\n",
    "# pool_size = 4\n",
    "\n",
    "# # LSTM\n",
    "# lstm_output_size = 70\n",
    "\n",
    "\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 90\n",
    "batch_size = 16\n",
    "#hidden_size = 352\n",
    "# remove tanh, no dropout\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# seq_length = 256  #Length of the sequence to be inserted into the LSTM\n",
    "# vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "# lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "ln = True # Layer Nomalization\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "fc = True # has fully connected layer\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "NNPC = Sequential()\n",
    "NNPC.add(LSTM(hidden_size, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC.add(LSTM(hidden_size, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC.add(LSTM(hidden_size, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC.add(LSTM(hidden_size, activation='tanh', stateful=True, batch_input_shape=(batch_size,tsteps,seg_len)))\n",
    "NNPC.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC.add(Dense(hidden_size))\n",
    "NNPC.add(Dense(n_symb))\n",
    "NNPC.add(Activation('softmax'))\n",
    "NNPC.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We fit different stateful models to get the ideas: Skip to actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((1000001, tsteps, seg_len)) # 64 characters context\n",
    "y = np.zeros((1000001, n_symb))\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(k,1000000+k+1): #len(s)):\n",
    "    train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "    train_target = s[idx2]\n",
    "    x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "    y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "    idx3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 139s 1ms/sample - loss: 1.4012 - categorical_accuracy: 0.2668 - val_loss: 1.3825 - val_categorical_accuracy: 0.2811\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 113s 1ms/sample - loss: 1.3805 - categorical_accuracy: 0.2845 - val_loss: 1.3808 - val_categorical_accuracy: 0.2784\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 116s 1ms/sample - loss: 1.3813 - categorical_accuracy: 0.2815 - val_loss: 1.3804 - val_categorical_accuracy: 0.2861\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 112s 1ms/sample - loss: 1.3831 - categorical_accuracy: 0.2783 - val_loss: 1.3760 - val_categorical_accuracy: 0.2959\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 112s 1ms/sample - loss: 1.3818 - categorical_accuracy: 0.2808 - val_loss: 1.3867 - val_categorical_accuracy: 0.2661\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 117s 1ms/sample - loss: 1.3856 - categorical_accuracy: 0.2663 - val_loss: 1.3836 - val_categorical_accuracy: 0.2727\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 112s 1ms/sample - loss: 1.3866 - categorical_accuracy: 0.2581 - val_loss: 1.3864 - val_categorical_accuracy: 0.2491\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 111s 1ms/sample - loss: 1.3870 - categorical_accuracy: 0.2548 - val_loss: 1.3861 - val_categorical_accuracy: 0.2658\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 114s 1ms/sample - loss: 1.3867 - categorical_accuracy: 0.2609 - val_loss: 1.3869 - val_categorical_accuracy: 0.2639\n"
     ]
    }
   ],
   "source": [
    "predicted_onehot = []\n",
    "#for i in range(len(ecoli)//n - 1):\n",
    "for i in range(9):\n",
    "#     if i%5 == 0:\n",
    "#         print(i)\n",
    "    NNPC.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "#     predicted_onehot += list(D1LSTM.predict_proba(x[n*(i+1):n*(i+2)]))\n",
    "# predicted_onehot += list(D1LSTM.predict_proba(x[n*(len(ecoli)//n):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (16, 20, 90)              39960     \n",
      "_________________________________________________________________\n",
      "layer_normalization_23 (Laye (16, 20, 90)              40        \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (16, 20, 90)              65160     \n",
      "_________________________________________________________________\n",
      "layer_normalization_24 (Laye (16, 20, 90)              40        \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (16, 20, 90)              65160     \n",
      "_________________________________________________________________\n",
      "layer_normalization_25 (Laye (16, 20, 90)              40        \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (16, 90)                  65160     \n",
      "_________________________________________________________________\n",
      "layer_normalization_26 (Laye (16, 90)                  180       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (16, 256)                 23296     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (16, 256)                 0         \n",
      "=================================================================\n",
      "Total params: 259,036\n",
      "Trainable params: 259,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNPC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 129s 1ms/sample - loss: 1.4775 - categorical_accuracy: 0.2544 - val_loss: 1.4091 - val_categorical_accuracy: 0.2407\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 121s 1ms/sample - loss: 1.4143 - categorical_accuracy: 0.2523 - val_loss: 1.4049 - val_categorical_accuracy: 0.2389\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 121s 1ms/sample - loss: 1.4102 - categorical_accuracy: 0.2532 - val_loss: 1.3960 - val_categorical_accuracy: 0.2479\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 119s 1ms/sample - loss: 1.4096 - categorical_accuracy: 0.2509 - val_loss: 1.3951 - val_categorical_accuracy: 0.2667\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 170s 2ms/sample - loss: 1.4090 - categorical_accuracy: 0.2529 - val_loss: 1.4049 - val_categorical_accuracy: 0.2606\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 209s 2ms/sample - loss: 1.4087 - categorical_accuracy: 0.2516 - val_loss: 1.4508 - val_categorical_accuracy: 0.2625\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 200s 2ms/sample - loss: 1.4092 - categorical_accuracy: 0.2497 - val_loss: 1.4198 - val_categorical_accuracy: 0.2624\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 200s 2ms/sample - loss: 1.4087 - categorical_accuracy: 0.2488 - val_loss: 1.4433 - val_categorical_accuracy: 0.2500\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 208s 2ms/sample - loss: 1.4078 - categorical_accuracy: 0.2527 - val_loss: 1.3994 - val_categorical_accuracy: 0.2639\n"
     ]
    }
   ],
   "source": [
    "NNPC2 = Sequential()\n",
    "\n",
    "NNPC2.add(LSTM(352, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC2.add(LSTM(352, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC2.add(LSTM(352, activation='tanh', stateful=True\n",
    "              , batch_input_shape=(batch_size,tsteps,seg_len),return_sequences=True))\n",
    "NNPC2.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(batch_size,tsteps,seg_len)))\n",
    "NNPC2.add(Dense(n_symb))\n",
    "NNPC2.add(Activation('softmax'))\n",
    "NNPC2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "for i in range(9):\n",
    "    NNPC2.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (16, 20, 352)             525184    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (16, 20, 352)             992640    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (16, 20, 352)             992640    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (16, 352)                 992640    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (16, 256)                 90368     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (16, 256)                 0         \n",
      "=================================================================\n",
      "Total params: 3,593,472\n",
      "Trainable params: 3,593,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNPC2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try actual implementation: 2 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 16s 157us/sample - loss: 1.4740 - categorical_accuracy: 0.2869 - val_loss: 1.3733 - val_categorical_accuracy: 0.3011\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 52us/sample - loss: 1.3710 - categorical_accuracy: 0.3093 - val_loss: 1.3687 - val_categorical_accuracy: 0.3075\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 55us/sample - loss: 1.3658 - categorical_accuracy: 0.3121 - val_loss: 1.3686 - val_categorical_accuracy: 0.3107\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 58us/sample - loss: 1.3597 - categorical_accuracy: 0.3206 - val_loss: 1.3722 - val_categorical_accuracy: 0.3028\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 56us/sample - loss: 1.3672 - categorical_accuracy: 0.3121 - val_loss: 1.3605 - val_categorical_accuracy: 0.3224\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 51us/sample - loss: 1.3631 - categorical_accuracy: 0.3166 - val_loss: 1.3581 - val_categorical_accuracy: 0.3233\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3586 - categorical_accuracy: 0.3238 - val_loss: 1.3597 - val_categorical_accuracy: 0.3228\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 54us/sample - loss: 1.3534 - categorical_accuracy: 0.3279 - val_loss: 1.3581 - val_categorical_accuracy: 0.3203\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 61us/sample - loss: 1.3448 - categorical_accuracy: 0.3424 - val_loss: 1.3543 - val_categorical_accuracy: 0.3325\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 7s 72us/sample - loss: 1.3395 - categorical_accuracy: 0.3486 - val_loss: 1.3515 - val_categorical_accuracy: 0.3354\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 5s 54us/sample - loss: 1.3507 - categorical_accuracy: 0.3363 - val_loss: 1.3465 - val_categorical_accuracy: 0.3386\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 56us/sample - loss: 1.3467 - categorical_accuracy: 0.3385 - val_loss: 1.3489 - val_categorical_accuracy: 0.3359\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 57us/sample - loss: 1.3464 - categorical_accuracy: 0.3382 - val_loss: 1.3433 - val_categorical_accuracy: 0.3409\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 56us/sample - loss: 1.3426 - categorical_accuracy: 0.3428 - val_loss: 1.3425 - val_categorical_accuracy: 0.3400\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 5s 54us/sample - loss: 1.3420 - categorical_accuracy: 0.3408 - val_loss: 1.3395 - val_categorical_accuracy: 0.3449\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 7s 65us/sample - loss: 1.3381 - categorical_accuracy: 0.3463 - val_loss: 1.3402 - val_categorical_accuracy: 0.3455\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 75us/sample - loss: 1.3380 - categorical_accuracy: 0.3486 - val_loss: 1.3399 - val_categorical_accuracy: 0.3483\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 54us/sample - loss: 1.3343 - categorical_accuracy: 0.3501 - val_loss: 1.3392 - val_categorical_accuracy: 0.3454\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 3s 33us/sample - loss: 1.3405 - categorical_accuracy: 0.3437\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 3s 34us/sample - loss: 1.3364 - categorical_accuracy: 0.3478\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.91 GiB for an array with shape (1000000, 256) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-33c98c818603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moverall\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 64 characters context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_symb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0midx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.91 GiB for an array with shape (1000000, 256) and data type float64"
     ]
    }
   ],
   "source": [
    "from plist import ProbabilityList\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,32)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_Py.txt'\n",
    "\n",
    "e_idx = 0\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model\n",
    "        \n",
    "    \n",
    "    for overall in range(5):\n",
    "        predicted_val = []\n",
    "        if overall <= 3:\n",
    "            x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((1000000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,1000000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 4:\n",
    "            x = np.zeros((len(s)-1000000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-1000000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=2)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            model.prob_list[:256] = prob_list\n",
    "            model.prob_list[256] = 1/100000\n",
    "            model.normalize()\n",
    "            t = model.get_total() ## New lines!\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "\n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to be careful! Stateful requires that we train/predict with shape divedable by the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s[:-40])-138250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138250"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "138290//250 * 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 23s 230us/sample - loss: 1.3970 - categorical_accuracy: 0.2870 - val_loss: 1.3752 - val_categorical_accuracy: 0.2986\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3691 - categorical_accuracy: 0.3083 - val_loss: 1.3732 - val_categorical_accuracy: 0.3022\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3645 - categorical_accuracy: 0.3133 - val_loss: 1.3689 - val_categorical_accuracy: 0.3042\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3572 - categorical_accuracy: 0.3243 - val_loss: 1.3697 - val_categorical_accuracy: 0.3104\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3624 - categorical_accuracy: 0.3155 - val_loss: 1.3593 - val_categorical_accuracy: 0.3218\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3571 - categorical_accuracy: 0.3238 - val_loss: 1.3555 - val_categorical_accuracy: 0.3240\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3536 - categorical_accuracy: 0.3277 - val_loss: 1.3448 - val_categorical_accuracy: 0.3401\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3489 - categorical_accuracy: 0.3334 - val_loss: 1.3479 - val_categorical_accuracy: 0.3345\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 93us/sample - loss: 1.3410 - categorical_accuracy: 0.3449 - val_loss: 1.3519 - val_categorical_accuracy: 0.3306\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3349 - categorical_accuracy: 0.3520 - val_loss: 1.3490 - val_categorical_accuracy: 0.3358\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3468 - categorical_accuracy: 0.3376 - val_loss: 1.3464 - val_categorical_accuracy: 0.3378\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3419 - categorical_accuracy: 0.3446 - val_loss: 1.3467 - val_categorical_accuracy: 0.3380\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3434 - categorical_accuracy: 0.3410 - val_loss: 1.3424 - val_categorical_accuracy: 0.3401\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3380 - categorical_accuracy: 0.3467 - val_loss: 1.3431 - val_categorical_accuracy: 0.3389\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 96us/sample - loss: 1.3390 - categorical_accuracy: 0.3447 - val_loss: 1.3359 - val_categorical_accuracy: 0.3491\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 90us/sample - loss: 1.3341 - categorical_accuracy: 0.3501 - val_loss: 1.3359 - val_categorical_accuracy: 0.3495\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3346 - categorical_accuracy: 0.3498 - val_loss: 1.3390 - val_categorical_accuracy: 0.3460\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 85us/sample - loss: 1.3300 - categorical_accuracy: 0.3556 - val_loss: 1.3361 - val_categorical_accuracy: 0.3479\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 62us/sample - loss: 1.3360 - categorical_accuracy: 0.3483\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 62us/sample - loss: 1.3309 - categorical_accuracy: 0.3540\n",
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3415 - categorical_accuracy: 0.3428 - val_loss: 1.3353 - val_categorical_accuracy: 0.3472\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3373 - categorical_accuracy: 0.3474 - val_loss: 1.3365 - val_categorical_accuracy: 0.3430\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 85us/sample - loss: 1.3357 - categorical_accuracy: 0.3464 - val_loss: 1.3467 - val_categorical_accuracy: 0.3358\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3310 - categorical_accuracy: 0.3513 - val_loss: 1.3447 - val_categorical_accuracy: 0.3374\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3439 - categorical_accuracy: 0.3380 - val_loss: 1.3369 - val_categorical_accuracy: 0.3466\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3394 - categorical_accuracy: 0.3438 - val_loss: 1.3387 - val_categorical_accuracy: 0.3423\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3358 - categorical_accuracy: 0.3477 - val_loss: 1.3420 - val_categorical_accuracy: 0.3414\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3310 - categorical_accuracy: 0.3522 - val_loss: 1.3458 - val_categorical_accuracy: 0.3363\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3409 - categorical_accuracy: 0.3408 - val_loss: 1.3453 - val_categorical_accuracy: 0.3368\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3361 - categorical_accuracy: 0.3475 - val_loss: 1.3478 - val_categorical_accuracy: 0.3351\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3430 - categorical_accuracy: 0.3406 - val_loss: 1.3438 - val_categorical_accuracy: 0.3429\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3384 - categorical_accuracy: 0.3425 - val_loss: 1.3447 - val_categorical_accuracy: 0.3428\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3429 - categorical_accuracy: 0.3440 - val_loss: 1.3365 - val_categorical_accuracy: 0.3476\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 80us/sample - loss: 1.3383 - categorical_accuracy: 0.3482 - val_loss: 1.3376 - val_categorical_accuracy: 0.3453\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 85us/sample - loss: 1.3360 - categorical_accuracy: 0.3464 - val_loss: 1.3362 - val_categorical_accuracy: 0.3503\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 79us/sample - loss: 1.3316 - categorical_accuracy: 0.3509 - val_loss: 1.3371 - val_categorical_accuracy: 0.3489\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 80us/sample - loss: 1.3360 - categorical_accuracy: 0.3500 - val_loss: 1.3351 - val_categorical_accuracy: 0.3500\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 11s 114us/sample - loss: 1.3314 - categorical_accuracy: 0.3519 - val_loss: 1.3344 - val_categorical_accuracy: 0.3506\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 7s 75us/sample - loss: 1.3325 - categorical_accuracy: 0.3517\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 79us/sample - loss: 1.3274 - categorical_accuracy: 0.3579\n",
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 120us/sample - loss: 1.3396 - categorical_accuracy: 0.3425 - val_loss: 1.3312 - val_categorical_accuracy: 0.3564\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 121us/sample - loss: 1.3345 - categorical_accuracy: 0.3473 - val_loss: 1.3322 - val_categorical_accuracy: 0.3541\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 13s 128us/sample - loss: 1.3295 - categorical_accuracy: 0.3567 - val_loss: 1.3264 - val_categorical_accuracy: 0.3626\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3243 - categorical_accuracy: 0.3620 - val_loss: 1.3298 - val_categorical_accuracy: 0.3552\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 117us/sample - loss: 1.3261 - categorical_accuracy: 0.3594 - val_loss: 1.3285 - val_categorical_accuracy: 0.3562\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3208 - categorical_accuracy: 0.3651 - val_loss: 1.3286 - val_categorical_accuracy: 0.3560\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 121us/sample - loss: 1.3273 - categorical_accuracy: 0.3575 - val_loss: 1.3399 - val_categorical_accuracy: 0.3462\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3224 - categorical_accuracy: 0.3641 - val_loss: 1.3424 - val_categorical_accuracy: 0.3419\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 119us/sample - loss: 1.3378 - categorical_accuracy: 0.3459 - val_loss: 1.3284 - val_categorical_accuracy: 0.3574\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 11s 115us/sample - loss: 1.3333 - categorical_accuracy: 0.3523 - val_loss: 1.3265 - val_categorical_accuracy: 0.3581\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 13s 129us/sample - loss: 1.3253 - categorical_accuracy: 0.3601 - val_loss: 1.3199 - val_categorical_accuracy: 0.3671\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 117us/sample - loss: 1.3204 - categorical_accuracy: 0.3649 - val_loss: 1.3215 - val_categorical_accuracy: 0.3655\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 121us/sample - loss: 1.3197 - categorical_accuracy: 0.3660 - val_loss: 1.3550 - val_categorical_accuracy: 0.3342\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3147 - categorical_accuracy: 0.3700 - val_loss: 1.3554 - val_categorical_accuracy: 0.3318\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 120us/sample - loss: 1.3454 - categorical_accuracy: 0.3385 - val_loss: 1.3283 - val_categorical_accuracy: 0.3575\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3408 - categorical_accuracy: 0.3421 - val_loss: 1.3298 - val_categorical_accuracy: 0.3544\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 12s 121us/sample - loss: 1.3274 - categorical_accuracy: 0.3579 - val_loss: 1.3378 - val_categorical_accuracy: 0.3463\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 11s 113us/sample - loss: 1.3226 - categorical_accuracy: 0.3612 - val_loss: 1.3360 - val_categorical_accuracy: 0.3508\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3338 - categorical_accuracy: 0.3536\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3279 - categorical_accuracy: 0.3557\n",
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3269 - categorical_accuracy: 0.3565 - val_loss: 1.3330 - val_categorical_accuracy: 0.3530\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3216 - categorical_accuracy: 0.3603 - val_loss: 1.3335 - val_categorical_accuracy: 0.3507\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 92us/sample - loss: 1.3308 - categorical_accuracy: 0.3530 - val_loss: 1.3287 - val_categorical_accuracy: 0.3529\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 93us/sample - loss: 1.3255 - categorical_accuracy: 0.3585 - val_loss: 1.3306 - val_categorical_accuracy: 0.3525\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3268 - categorical_accuracy: 0.3574 - val_loss: 1.3271 - val_categorical_accuracy: 0.3588\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 100us/sample - loss: 1.3212 - categorical_accuracy: 0.3639 - val_loss: 1.3296 - val_categorical_accuracy: 0.3554\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3247 - categorical_accuracy: 0.3581 - val_loss: 1.3388 - val_categorical_accuracy: 0.3430\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 95us/sample - loss: 1.3188 - categorical_accuracy: 0.3645 - val_loss: 1.3398 - val_categorical_accuracy: 0.3401\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3360 - categorical_accuracy: 0.3436 - val_loss: 1.3299 - val_categorical_accuracy: 0.3537\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 99us/sample - loss: 1.3303 - categorical_accuracy: 0.3496 - val_loss: 1.3265 - val_categorical_accuracy: 0.3577\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 11s 106us/sample - loss: 1.3212 - categorical_accuracy: 0.3637 - val_loss: 1.3267 - val_categorical_accuracy: 0.3592\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3155 - categorical_accuracy: 0.3680 - val_loss: 1.3267 - val_categorical_accuracy: 0.3586\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3244 - categorical_accuracy: 0.3607 - val_loss: 1.3294 - val_categorical_accuracy: 0.3534\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 96us/sample - loss: 1.3183 - categorical_accuracy: 0.3675 - val_loss: 1.3328 - val_categorical_accuracy: 0.3501\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 95us/sample - loss: 1.3288 - categorical_accuracy: 0.3535 - val_loss: 1.3268 - val_categorical_accuracy: 0.3586\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 92us/sample - loss: 1.3239 - categorical_accuracy: 0.3588 - val_loss: 1.3267 - val_categorical_accuracy: 0.3567\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 101us/sample - loss: 1.3259 - categorical_accuracy: 0.3585 - val_loss: 1.3286 - val_categorical_accuracy: 0.3547\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 96us/sample - loss: 1.3208 - categorical_accuracy: 0.3650 - val_loss: 1.3281 - val_categorical_accuracy: 0.3556\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 7s 66us/sample - loss: 1.3272 - categorical_accuracy: 0.3564s - loss: 1.3276 - categorical_accu\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 7s 65us/sample - loss: 1.3223 - categorical_accuracy: 0.3609\n",
      "4638690\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 91us/sample - loss: 1.3287 - categorical_accuracy: 0.3586 - val_loss: 1.3291 - val_categorical_accuracy: 0.3564\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 90us/sample - loss: 1.3242 - categorical_accuracy: 0.3618 - val_loss: 1.3277 - val_categorical_accuracy: 0.3579\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 95us/sample - loss: 1.3267 - categorical_accuracy: 0.3590 - val_loss: 1.3272 - val_categorical_accuracy: 0.3586\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3223 - categorical_accuracy: 0.3639 - val_loss: 1.3282 - val_categorical_accuracy: 0.3559\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3257 - categorical_accuracy: 0.3573 - val_loss: 1.3267 - val_categorical_accuracy: 0.3617\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3199 - categorical_accuracy: 0.3633 - val_loss: 1.3263 - val_categorical_accuracy: 0.3626\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3213 - categorical_accuracy: 0.3647 - val_loss: 1.3315 - val_categorical_accuracy: 0.3539\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3155 - categorical_accuracy: 0.3698 - val_loss: 1.3310 - val_categorical_accuracy: 0.3534\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 138290 samples",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ba7360711fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                   validation_data=(x[n*(i+1):], y[n*(i+1):]))\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mpredicted_val\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNNPC3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    562\u001b[0m                                     \u001b[0mclass_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                                     \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m                                     distribution_strategy=distribution_strategy)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       raise ValueError('`validation_steps` should not be specified if '\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2558\u001b[0m                          \u001b[1;34m'a number of samples that can be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2559\u001b[0m                          \u001b[1;34m'divided by the batch size. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2560\u001b[1;33m                          str(x[0].shape[0]) + ' samples')\n\u001b[0m\u001b[0;32m   2561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2562\u001b[0m     \u001b[1;31m# If dictionary inputs were provided, we return a dictionary as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 138290 samples"
     ]
    }
   ],
   "source": [
    "from plist import ProbabilityList\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,32)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_Py.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model\n",
    "        \n",
    "    \n",
    "    for overall in range(5):\n",
    "        predicted_val = []\n",
    "        if overall <= 3:\n",
    "            x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((1000000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,1000000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 4:\n",
    "            x = np.zeros((len(s)-1000000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-1000000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=2)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            model.prob_list[:4] = prob_list\n",
    "            model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "            model.normalize()\n",
    "            t = model.get_total() ## New lines!\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "\n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 26s 257us/sample - loss: 1.3981 - categorical_accuracy: 0.2844 - val_loss: 1.3813 - val_categorical_accuracy: 0.2912\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 99us/sample - loss: 1.3683 - categorical_accuracy: 0.3123 - val_loss: 1.3711 - val_categorical_accuracy: 0.3061\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 99us/sample - loss: 1.3648 - categorical_accuracy: 0.3115 - val_loss: 1.3668 - val_categorical_accuracy: 0.3099\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 98us/sample - loss: 1.3549 - categorical_accuracy: 0.3254 - val_loss: 1.3651 - val_categorical_accuracy: 0.3160\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 97us/sample - loss: 1.3598 - categorical_accuracy: 0.3209 - val_loss: 1.3570 - val_categorical_accuracy: 0.3270\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3540 - categorical_accuracy: 0.3289 - val_loss: 1.3535 - val_categorical_accuracy: 0.3282\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3513 - categorical_accuracy: 0.3311 - val_loss: 1.3440 - val_categorical_accuracy: 0.3401\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3467 - categorical_accuracy: 0.3365 - val_loss: 1.3434 - val_categorical_accuracy: 0.3411\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3398 - categorical_accuracy: 0.3458 - val_loss: 1.3508 - val_categorical_accuracy: 0.3337\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3346 - categorical_accuracy: 0.3519 - val_loss: 1.3476 - val_categorical_accuracy: 0.3378\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 80us/sample - loss: 1.3459 - categorical_accuracy: 0.3392 - val_loss: 1.3453 - val_categorical_accuracy: 0.3385\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 77us/sample - loss: 1.3418 - categorical_accuracy: 0.3417 - val_loss: 1.3450 - val_categorical_accuracy: 0.3395\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3427 - categorical_accuracy: 0.3411 - val_loss: 1.3412 - val_categorical_accuracy: 0.3415\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 76us/sample - loss: 1.3374 - categorical_accuracy: 0.3468 - val_loss: 1.3440 - val_categorical_accuracy: 0.3388\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3382 - categorical_accuracy: 0.3450 - val_loss: 1.3383 - val_categorical_accuracy: 0.3461\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 77us/sample - loss: 1.3330 - categorical_accuracy: 0.3507 - val_loss: 1.3363 - val_categorical_accuracy: 0.3502\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3352 - categorical_accuracy: 0.3493 - val_loss: 1.3376 - val_categorical_accuracy: 0.3470\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 76us/sample - loss: 1.3298 - categorical_accuracy: 0.3560 - val_loss: 1.3372 - val_categorical_accuracy: 0.3452\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 5s 49us/sample - loss: 1.3356 - categorical_accuracy: 0.3495\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3306 - categorical_accuracy: 0.3533\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 94us/sample - loss: 1.3407 - categorical_accuracy: 0.3431 - val_loss: 1.3369 - val_categorical_accuracy: 0.3455\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 85us/sample - loss: 1.3365 - categorical_accuracy: 0.3466 - val_loss: 1.3380 - val_categorical_accuracy: 0.3458\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 90us/sample - loss: 1.3356 - categorical_accuracy: 0.3460 - val_loss: 1.3462 - val_categorical_accuracy: 0.3350\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3311 - categorical_accuracy: 0.3513 - val_loss: 1.3453 - val_categorical_accuracy: 0.3341\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3441 - categorical_accuracy: 0.3377 - val_loss: 1.3360 - val_categorical_accuracy: 0.3480\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3402 - categorical_accuracy: 0.3428 - val_loss: 1.3374 - val_categorical_accuracy: 0.3476\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3351 - categorical_accuracy: 0.3496 - val_loss: 1.3426 - val_categorical_accuracy: 0.3398\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 75us/sample - loss: 1.3307 - categorical_accuracy: 0.3540 - val_loss: 1.3418 - val_categorical_accuracy: 0.3404\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 79us/sample - loss: 1.3408 - categorical_accuracy: 0.3418 - val_loss: 1.3448 - val_categorical_accuracy: 0.3348\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 77us/sample - loss: 1.3362 - categorical_accuracy: 0.3471 - val_loss: 1.3472 - val_categorical_accuracy: 0.3350\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3426 - categorical_accuracy: 0.3381 - val_loss: 1.3445 - val_categorical_accuracy: 0.3431\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 76us/sample - loss: 1.3380 - categorical_accuracy: 0.3439 - val_loss: 1.3443 - val_categorical_accuracy: 0.3395\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 7s 73us/sample - loss: 1.3423 - categorical_accuracy: 0.3440 - val_loss: 1.3356 - val_categorical_accuracy: 0.3475\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3379 - categorical_accuracy: 0.3491 - val_loss: 1.3383 - val_categorical_accuracy: 0.3432\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3359 - categorical_accuracy: 0.3484 - val_loss: 1.3359 - val_categorical_accuracy: 0.3474\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3313 - categorical_accuracy: 0.3531 - val_loss: 1.3358 - val_categorical_accuracy: 0.3479\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3364 - categorical_accuracy: 0.3478 - val_loss: 1.3338 - val_categorical_accuracy: 0.3504\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3318 - categorical_accuracy: 0.3527 - val_loss: 1.3335 - val_categorical_accuracy: 0.3497\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 57us/sample - loss: 1.3322 - categorical_accuracy: 0.3537\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 55us/sample - loss: 1.3271 - categorical_accuracy: 0.3581\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 92us/sample - loss: 1.3400 - categorical_accuracy: 0.3419 - val_loss: 1.3310 - val_categorical_accuracy: 0.3553\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 85us/sample - loss: 1.3349 - categorical_accuracy: 0.3482 - val_loss: 1.3326 - val_categorical_accuracy: 0.3528\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3293 - categorical_accuracy: 0.3548 - val_loss: 1.3284 - val_categorical_accuracy: 0.3607\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 80us/sample - loss: 1.3240 - categorical_accuracy: 0.3610 - val_loss: 1.3285 - val_categorical_accuracy: 0.3595\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3267 - categorical_accuracy: 0.3607 - val_loss: 1.3275 - val_categorical_accuracy: 0.3575\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3219 - categorical_accuracy: 0.3653 - val_loss: 1.3275 - val_categorical_accuracy: 0.3589\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3271 - categorical_accuracy: 0.3593 - val_loss: 1.3389 - val_categorical_accuracy: 0.3457\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3221 - categorical_accuracy: 0.3634 - val_loss: 1.3401 - val_categorical_accuracy: 0.3432\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 77us/sample - loss: 1.3379 - categorical_accuracy: 0.3479 - val_loss: 1.3262 - val_categorical_accuracy: 0.3595\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3333 - categorical_accuracy: 0.3522 - val_loss: 1.3287 - val_categorical_accuracy: 0.3561\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3263 - categorical_accuracy: 0.3584 - val_loss: 1.3203 - val_categorical_accuracy: 0.3665\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 7s 69us/sample - loss: 1.3213 - categorical_accuracy: 0.3645 - val_loss: 1.3206 - val_categorical_accuracy: 0.3662\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 7s 75us/sample - loss: 1.3193 - categorical_accuracy: 0.3665 - val_loss: 1.3510 - val_categorical_accuracy: 0.3348\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 7s 72us/sample - loss: 1.3146 - categorical_accuracy: 0.3703 - val_loss: 1.3514 - val_categorical_accuracy: 0.3343\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 77us/sample - loss: 1.3460 - categorical_accuracy: 0.3373 - val_loss: 1.3291 - val_categorical_accuracy: 0.3564\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3417 - categorical_accuracy: 0.3423 - val_loss: 1.3322 - val_categorical_accuracy: 0.3519\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 79us/sample - loss: 1.3273 - categorical_accuracy: 0.3574 - val_loss: 1.3360 - val_categorical_accuracy: 0.3500\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3228 - categorical_accuracy: 0.3620 - val_loss: 1.3369 - val_categorical_accuracy: 0.3489\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 5s 53us/sample - loss: 1.3344 - categorical_accuracy: 0.3521\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 5s 49us/sample - loss: 1.3294 - categorical_accuracy: 0.3572\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3267 - categorical_accuracy: 0.3560 - val_loss: 1.3319 - val_categorical_accuracy: 0.3520\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3225 - categorical_accuracy: 0.3600 - val_loss: 1.3345 - val_categorical_accuracy: 0.3495\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3305 - categorical_accuracy: 0.3550 - val_loss: 1.3294 - val_categorical_accuracy: 0.3544\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3254 - categorical_accuracy: 0.3602 - val_loss: 1.3318 - val_categorical_accuracy: 0.3514\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3264 - categorical_accuracy: 0.3587 - val_loss: 1.3275 - val_categorical_accuracy: 0.3593\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3214 - categorical_accuracy: 0.3638 - val_loss: 1.3312 - val_categorical_accuracy: 0.3513\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3232 - categorical_accuracy: 0.3596 - val_loss: 1.3373 - val_categorical_accuracy: 0.3436\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3175 - categorical_accuracy: 0.3642 - val_loss: 1.3406 - val_categorical_accuracy: 0.3434\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 82us/sample - loss: 1.3360 - categorical_accuracy: 0.3433 - val_loss: 1.3302 - val_categorical_accuracy: 0.3552\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 85us/sample - loss: 1.3308 - categorical_accuracy: 0.3490 - val_loss: 1.3306 - val_categorical_accuracy: 0.3566\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3215 - categorical_accuracy: 0.3624 - val_loss: 1.3269 - val_categorical_accuracy: 0.3589\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3158 - categorical_accuracy: 0.3697 - val_loss: 1.3268 - val_categorical_accuracy: 0.3582\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3242 - categorical_accuracy: 0.3628 - val_loss: 1.3335 - val_categorical_accuracy: 0.3497\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3182 - categorical_accuracy: 0.3683 - val_loss: 1.3302 - val_categorical_accuracy: 0.3531\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3294 - categorical_accuracy: 0.3536 - val_loss: 1.3275 - val_categorical_accuracy: 0.3602\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3242 - categorical_accuracy: 0.3576 - val_loss: 1.3273 - val_categorical_accuracy: 0.3613\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3262 - categorical_accuracy: 0.3602 - val_loss: 1.3306 - val_categorical_accuracy: 0.3539\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3207 - categorical_accuracy: 0.3655 - val_loss: 1.3331 - val_categorical_accuracy: 0.3526\n",
      "Train on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 6s 56us/sample - loss: 1.3280 - categorical_accuracy: 0.3573\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 6s 56us/sample - loss: 1.3225 - categorical_accuracy: 0.3631\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 85us/sample - loss: 1.3288 - categorical_accuracy: 0.3590 - val_loss: 1.3277 - val_categorical_accuracy: 0.3570\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3242 - categorical_accuracy: 0.3622 - val_loss: 1.3286 - val_categorical_accuracy: 0.3572\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 84us/sample - loss: 1.3264 - categorical_accuracy: 0.3603 - val_loss: 1.3262 - val_categorical_accuracy: 0.3592\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 86us/sample - loss: 1.3213 - categorical_accuracy: 0.3631 - val_loss: 1.3319 - val_categorical_accuracy: 0.3553\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 8s 81us/sample - loss: 1.3263 - categorical_accuracy: 0.3578 - val_loss: 1.3248 - val_categorical_accuracy: 0.3614\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 8s 83us/sample - loss: 1.3203 - categorical_accuracy: 0.3639 - val_loss: 1.3264 - val_categorical_accuracy: 0.3596\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 9s 95us/sample - loss: 1.3219 - categorical_accuracy: 0.3655 - val_loss: 1.3300 - val_categorical_accuracy: 0.3540\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 9s 87us/sample - loss: 1.3165 - categorical_accuracy: 0.3701 - val_loss: 1.3293 - val_categorical_accuracy: 0.3549\n",
      "Train on 100000 samples, validate on 138250 samples\n",
      "Epoch 1/2\n",
      "100000/100000 [==============================] - 10s 98us/sample - loss: 1.3264 - categorical_accuracy: 0.3573 - val_loss: 1.3321 - val_categorical_accuracy: 0.3532\n",
      "Epoch 2/2\n",
      "100000/100000 [==============================] - 10s 99us/sample - loss: 1.3210 - categorical_accuracy: 0.3641 - val_loss: 1.3343 - val_categorical_accuracy: 0.3513\n",
      "Train on 138250 samples\n",
      "Epoch 1/2\n",
      "138250/138250 [==============================] - 8s 55us/sample - loss: 1.3328 - categorical_accuracy: 0.3529\n",
      "Epoch 2/2\n",
      "138250/138250 [==============================] - 7s 49us/sample - loss: 1.3285 - categorical_accuracy: 0.3571\n"
     ]
    }
   ],
   "source": [
    "from plist import ProbabilityList\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,32), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,32), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(32, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,32)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_Py2.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(5):\n",
    "        predicted_val = []\n",
    "        if overall <= 3:\n",
    "            x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((1000000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,1000000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 4:\n",
    "            x = np.zeros((len(s)-1000000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-1000000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(1000000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=2,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=2)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    \n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s[:-50])-138240 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "138250//16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 50s 496us/sample - loss: 1.5899 - categorical_accuracy: 0.2582 - val_loss: 1.4256 - val_categorical_accuracy: 0.2689\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 32s 322us/sample - loss: 1.4341 - categorical_accuracy: 0.2586 - val_loss: 1.4302 - val_categorical_accuracy: 0.2631\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 32s 322us/sample - loss: 1.4119 - categorical_accuracy: 0.2624 - val_loss: 1.4203 - val_categorical_accuracy: 0.2634\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 32s 316us/sample - loss: 1.3961 - categorical_accuracy: 0.2722 - val_loss: 1.3943 - val_categorical_accuracy: 0.2851\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 23s 231us/sample - loss: 1.3814 - categorical_accuracy: 0.2926\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 31s 308us/sample - loss: 1.3749 - categorical_accuracy: 0.2990 - val_loss: 1.3658 - val_categorical_accuracy: 0.3141\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3643 - categorical_accuracy: 0.3137 - val_loss: 1.3611 - val_categorical_accuracy: 0.3221\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3537 - categorical_accuracy: 0.3283 - val_loss: 1.3498 - val_categorical_accuracy: 0.3364\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3470 - categorical_accuracy: 0.3360 - val_loss: 1.3521 - val_categorical_accuracy: 0.3291\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 194us/sample - loss: 1.3467 - categorical_accuracy: 0.3385\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3512 - categorical_accuracy: 0.3333 - val_loss: 1.3470 - val_categorical_accuracy: 0.3357\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3452 - categorical_accuracy: 0.3360 - val_loss: 1.3530 - val_categorical_accuracy: 0.3271 0.\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3534 - categorical_accuracy: 0.3272 - val_loss: 1.3489 - val_categorical_accuracy: 0.3336\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3439 - categorical_accuracy: 0.3395 - val_loss: 1.3536 - val_categorical_accuracy: 0.3277\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 194us/sample - loss: 1.3486 - categorical_accuracy: 0.3331\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3502 - categorical_accuracy: 0.3298 - val_loss: 1.3523 - val_categorical_accuracy: 0.3343\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3502 - categorical_accuracy: 0.3321 - val_loss: 1.3477 - val_categorical_accuracy: 0.3334\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3448 - categorical_accuracy: 0.3380 - val_loss: 1.3449 - val_categorical_accuracy: 0.3405\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 265us/sample - loss: 1.3455 - categorical_accuracy: 0.3401 - val_loss: 1.3478 - val_categorical_accuracy: 0.3333\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 20s 195us/sample - loss: 1.3417 - categorical_accuracy: 0.3419\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3490 - categorical_accuracy: 0.3332 - val_loss: 1.3424 - val_categorical_accuracy: 0.3422\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3416 - categorical_accuracy: 0.3404 - val_loss: 1.3400 - val_categorical_accuracy: 0.3487\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3383 - categorical_accuracy: 0.3484 - val_loss: 1.3383 - val_categorical_accuracy: 0.3468\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3393 - categorical_accuracy: 0.3453 - val_loss: 1.3510 - val_categorical_accuracy: 0.3341\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 20s 196us/sample - loss: 1.3485 - categorical_accuracy: 0.3357- loss: 1.3485 - categorica\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3376 - categorical_accuracy: 0.3487 - val_loss: 1.3365 - val_categorical_accuracy: 0.3458\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3341 - categorical_accuracy: 0.3513 - val_loss: 1.3582 - val_categorical_accuracy: 0.3244\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3555 - categorical_accuracy: 0.3259 - val_loss: 1.3418 - val_categorical_accuracy: 0.3413\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3419 - categorical_accuracy: 0.3401 - val_loss: 1.3478 - val_categorical_accuracy: 0.3354ca\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 195us/sample - loss: 1.3484 - categorical_accuracy: 0.3357\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 260us/sample - loss: 1.3402 - categorical_accuracy: 0.3422 - val_loss: 1.3493 - val_categorical_accuracy: 0.3313\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3434 - categorical_accuracy: 0.3401 - val_loss: 1.3477 - val_categorical_accuracy: 0.3331\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3415 - categorical_accuracy: 0.3416 - val_loss: 1.3434 - val_categorical_accuracy: 0.3433\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3410 - categorical_accuracy: 0.3413 - val_loss: 1.3583 - val_categorical_accuracy: 0.3221\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 20s 196us/sample - loss: 1.3516 - categorical_accuracy: 0.3261\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3413 - categorical_accuracy: 0.3442 - val_loss: 1.3466 - val_categorical_accuracy: 0.3382\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3457 - categorical_accuracy: 0.3370 - val_loss: 1.3496 - val_categorical_accuracy: 0.3319\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3509 - categorical_accuracy: 0.3324 - val_loss: 1.3489 - val_categorical_accuracy: 0.3348\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3482 - categorical_accuracy: 0.3352 - val_loss: 1.3485 - val_categorical_accuracy: 0.3354\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 20s 196us/sample - loss: 1.3478 - categorical_accuracy: 0.3334- loss: 1.3480 - categorical_ac - ETA: 0s - loss: 1.3478 - categorical_accu\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 261us/sample - loss: 1.3532 - categorical_accuracy: 0.3322 - val_loss: 1.3486 - val_categorical_accuracy: 0.3376\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 262us/sample - loss: 1.3486 - categorical_accuracy: 0.3356 - val_loss: 1.3507 - val_categorical_accuracy: 0.3322\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 263us/sample - loss: 1.3505 - categorical_accuracy: 0.3329 - val_loss: 1.3564 - val_categorical_accuracy: 0.3312\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 264us/sample - loss: 1.3493 - categorical_accuracy: 0.3352 - val_loss: 1.3539 - val_categorical_accuracy: 0.3285\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 20s 196us/sample - loss: 1.3541 - categorical_accuracy: 0.3280\n",
      "4638650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from plist import ProbabilityList\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_PyExact.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(10):\n",
    "        predicted_val = []\n",
    "        if overall <= 8:\n",
    "            x = np.zeros((500000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((500000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,500000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 9:\n",
    "            x = np.zeros((len(s)-500000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-500000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=1)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    \n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 35s 350us/sample - loss: 1.6082 - categorical_accuracy: 0.2581 - val_loss: 1.4942 - val_categorical_accuracy: 0.2500\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 256us/sample - loss: 1.4315 - categorical_accuracy: 0.2615 - val_loss: 1.4042 - val_categorical_accuracy: 0.2646\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 255us/sample - loss: 1.4060 - categorical_accuracy: 0.2664 - val_loss: 1.3976 - val_categorical_accuracy: 0.2724\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 254us/sample - loss: 1.3926 - categorical_accuracy: 0.2823 - val_loss: 1.3760 - val_categorical_accuracy: 0.3008\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 188us/sample - loss: 1.3790 - categorical_accuracy: 0.2994\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3742 - categorical_accuracy: 0.3033 - val_loss: 1.3774 - val_categorical_accuracy: 0.3054\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3648 - categorical_accuracy: 0.3155 - val_loss: 1.3593 - val_categorical_accuracy: 0.3218\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3565 - categorical_accuracy: 0.3272 - val_loss: 1.3484 - val_categorical_accuracy: 0.3356\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 254us/sample - loss: 1.3474 - categorical_accuracy: 0.3363 - val_loss: 1.3505 - val_categorical_accuracy: 0.3301\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 189us/sample - loss: 1.3468 - categorical_accuracy: 0.3372\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3509 - categorical_accuracy: 0.3320 - val_loss: 1.3461 - val_categorical_accuracy: 0.3362\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3445 - categorical_accuracy: 0.3363 - val_loss: 1.3543 - val_categorical_accuracy: 0.3229\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3533 - categorical_accuracy: 0.3274 - val_loss: 1.3481 - val_categorical_accuracy: 0.3338\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 255us/sample - loss: 1.3441 - categorical_accuracy: 0.3375 - val_loss: 1.3492 - val_categorical_accuracy: 0.3362\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 189us/sample - loss: 1.3495 - categorical_accuracy: 0.3325\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3505 - categorical_accuracy: 0.3293 - val_loss: 1.3517 - val_categorical_accuracy: 0.3283\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3507 - categorical_accuracy: 0.3331 - val_loss: 1.3491 - val_categorical_accuracy: 0.3301\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3451 - categorical_accuracy: 0.3363 - val_loss: 1.3460 - val_categorical_accuracy: 0.3377\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 26s 256us/sample - loss: 1.3447 - categorical_accuracy: 0.3389 - val_loss: 1.3447 - val_categorical_accuracy: 0.3388\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 188us/sample - loss: 1.3412 - categorical_accuracy: 0.3430\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 249us/sample - loss: 1.3482 - categorical_accuracy: 0.3337 - val_loss: 1.3453 - val_categorical_accuracy: 0.3387\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 251us/sample - loss: 1.3414 - categorical_accuracy: 0.3425 - val_loss: 1.3408 - val_categorical_accuracy: 0.3444\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 251us/sample - loss: 1.3373 - categorical_accuracy: 0.3485 - val_loss: 1.3397 - val_categorical_accuracy: 0.3447\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3386 - categorical_accuracy: 0.3446 - val_loss: 1.3506 - val_categorical_accuracy: 0.3302\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 187us/sample - loss: 1.3481 - categorical_accuracy: 0.3339\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 250us/sample - loss: 1.3365 - categorical_accuracy: 0.3497 - val_loss: 1.3340 - val_categorical_accuracy: 0.3492\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3322 - categorical_accuracy: 0.3530 - val_loss: 1.3553 - val_categorical_accuracy: 0.3283\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3539 - categorical_accuracy: 0.3261 - val_loss: 1.3436 - val_categorical_accuracy: 0.3386\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3389 - categorical_accuracy: 0.3434 - val_loss: 1.3452 - val_categorical_accuracy: 0.3389\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 188us/sample - loss: 1.3451 - categorical_accuracy: 0.3406\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 250us/sample - loss: 1.3386 - categorical_accuracy: 0.3442 - val_loss: 1.3444 - val_categorical_accuracy: 0.3398\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3432 - categorical_accuracy: 0.3392 - val_loss: 1.3416 - val_categorical_accuracy: 0.3406\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3389 - categorical_accuracy: 0.3437 - val_loss: 1.3382 - val_categorical_accuracy: 0.3467\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3376 - categorical_accuracy: 0.3480 - val_loss: 1.3512 - val_categorical_accuracy: 0.3311\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 187us/sample - loss: 1.3480 - categorical_accuracy: 0.3307\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 251us/sample - loss: 1.3374 - categorical_accuracy: 0.3479 - val_loss: 1.3440 - val_categorical_accuracy: 0.3401\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3417 - categorical_accuracy: 0.3429 - val_loss: 1.3478 - val_categorical_accuracy: 0.3354\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 252us/sample - loss: 1.3489 - categorical_accuracy: 0.3320 - val_loss: 1.3488 - val_categorical_accuracy: 0.3347\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.3458 - categorical_accuracy: 0.3400 - val_loss: 1.3505 - val_categorical_accuracy: 0.3314\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 19s 187us/sample - loss: 1.3440 - categorical_accuracy: 0.3413\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.49 GiB for an array with shape (500000, 20, 20) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e4362e213657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mpredicted_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moverall\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 64 characters context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_symb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.49 GiB for an array with shape (500000, 20, 20) and data type float64"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from plist import ProbabilityList\n",
    "\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        ecoli = inner_list\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s =  [temp_dict[i] for i in ecoli]\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "update_period = len(s)\n",
    "\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 20 #time steps\n",
    "seg_len = 20 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 256\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 90\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "ln = True # Layer Nomalization\n",
    "\n",
    "fc = True # has fully connected layer\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_PyExact.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(10):\n",
    "        predicted_val = []\n",
    "        if overall <= 8:\n",
    "            x = np.zeros((500000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((500000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,500000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 9:\n",
    "            x = np.zeros((len(s)-500000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-500000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=1)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    \n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 25s 253us/sample - loss: 1.5668 - categorical_accuracy: 0.2740 - val_loss: 1.4720 - val_categorical_accuracy: 0.2455\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3995 - categorical_accuracy: 0.2878 - val_loss: 1.4006 - val_categorical_accuracy: 0.2860\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3804 - categorical_accuracy: 0.2984 - val_loss: 1.3656 - val_categorical_accuracy: 0.3131\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 125us/sample - loss: 1.3682 - categorical_accuracy: 0.3155 - val_loss: 1.3561 - val_categorical_accuracy: 0.3265\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3590 - categorical_accuracy: 0.3263\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 121us/sample - loss: 1.3624 - categorical_accuracy: 0.3197 - val_loss: 1.3710 - val_categorical_accuracy: 0.3149\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3556 - categorical_accuracy: 0.3267 - val_loss: 1.3542 - val_categorical_accuracy: 0.3284\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3529 - categorical_accuracy: 0.3303 - val_loss: 1.3528 - val_categorical_accuracy: 0.3289\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3483 - categorical_accuracy: 0.3351 - val_loss: 1.3470 - val_categorical_accuracy: 0.3367\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3487 - categorical_accuracy: 0.3350\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3527 - categorical_accuracy: 0.3305 - val_loss: 1.3489 - val_categorical_accuracy: 0.3335\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3466 - categorical_accuracy: 0.3343 - val_loss: 1.3567 - val_categorical_accuracy: 0.3210\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3537 - categorical_accuracy: 0.3281 - val_loss: 1.3458 - val_categorical_accuracy: 0.3362\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3447 - categorical_accuracy: 0.3399 - val_loss: 1.3482 - val_categorical_accuracy: 0.3356\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3493 - categorical_accuracy: 0.3344\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 124us/sample - loss: 1.3523 - categorical_accuracy: 0.3302 - val_loss: 1.3506 - val_categorical_accuracy: 0.3312\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 124us/sample - loss: 1.3500 - categorical_accuracy: 0.3352 - val_loss: 1.3479 - val_categorical_accuracy: 0.3332\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 124us/sample - loss: 1.3449 - categorical_accuracy: 0.3369 - val_loss: 1.3445 - val_categorical_accuracy: 0.3384\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 125us/sample - loss: 1.3447 - categorical_accuracy: 0.3390 - val_loss: 1.3440 - val_categorical_accuracy: 0.3399\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 90us/sample - loss: 1.3421 - categorical_accuracy: 0.3421\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3489 - categorical_accuracy: 0.3332 - val_loss: 1.3422 - val_categorical_accuracy: 0.3428\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3408 - categorical_accuracy: 0.3437 - val_loss: 1.3388 - val_categorical_accuracy: 0.3473\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3369 - categorical_accuracy: 0.3515 - val_loss: 1.3421 - val_categorical_accuracy: 0.3437\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3376 - categorical_accuracy: 0.3491 - val_loss: 1.3503 - val_categorical_accuracy: 0.3346\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3468 - categorical_accuracy: 0.3379\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3348 - categorical_accuracy: 0.3516 - val_loss: 1.3319 - val_categorical_accuracy: 0.3533\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3305 - categorical_accuracy: 0.3551 - val_loss: 1.3534 - val_categorical_accuracy: 0.3321\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3528 - categorical_accuracy: 0.3307 - val_loss: 1.3441 - val_categorical_accuracy: 0.3402\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3361 - categorical_accuracy: 0.3461 - val_loss: 1.3416 - val_categorical_accuracy: 0.3454\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3430 - categorical_accuracy: 0.3428\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3343 - categorical_accuracy: 0.3511 - val_loss: 1.3394 - val_categorical_accuracy: 0.3439\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 126us/sample - loss: 1.3394 - categorical_accuracy: 0.3444 - val_loss: 1.3397 - val_categorical_accuracy: 0.3441\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 126us/sample - loss: 1.3361 - categorical_accuracy: 0.3485 - val_loss: 1.3343 - val_categorical_accuracy: 0.3524\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 128us/sample - loss: 1.3319 - categorical_accuracy: 0.3537 - val_loss: 1.3447 - val_categorical_accuracy: 0.3375\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 91us/sample - loss: 1.3428 - categorical_accuracy: 0.3366\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3309 - categorical_accuracy: 0.3546 - val_loss: 1.3394 - val_categorical_accuracy: 0.3484\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3360 - categorical_accuracy: 0.3498 - val_loss: 1.3406 - val_categorical_accuracy: 0.3431\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3396 - categorical_accuracy: 0.3446 - val_loss: 1.3383 - val_categorical_accuracy: 0.3495\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3351 - categorical_accuracy: 0.3503 - val_loss: 1.3454 - val_categorical_accuracy: 0.3384\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 88us/sample - loss: 1.3357 - categorical_accuracy: 0.3474\n",
      "4638650\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 122us/sample - loss: 1.3378 - categorical_accuracy: 0.3488 - val_loss: 1.3380 - val_categorical_accuracy: 0.3441\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3358 - categorical_accuracy: 0.3490 - val_loss: 1.3394 - val_categorical_accuracy: 0.3451\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 123us/sample - loss: 1.3363 - categorical_accuracy: 0.3473 - val_loss: 1.3354 - val_categorical_accuracy: 0.3527\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 12s 124us/sample - loss: 1.3314 - categorical_accuracy: 0.3549 - val_loss: 1.3432 - val_categorical_accuracy: 0.3431\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 9s 89us/sample - loss: 1.3364 - categorical_accuracy: 0.3468\n",
      "4638650\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from plist import ProbabilityList\n",
    "\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        ecoli = inner_list\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s =  [temp_dict[i] for i in ecoli]\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "update_period = len(s)\n",
    "\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 20 #time steps\n",
    "seg_len = 20 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 256\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 90\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "ln = True # Layer Nomalization\n",
    "\n",
    "fc = True # has fully connected layer\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,seg_len), return_sequences=True))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,352)))\n",
    "NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_NNPC_PyExact.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [1/257 for i in range(257)]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(10):\n",
    "        predicted_val = []\n",
    "        if overall <= 8:\n",
    "            x = np.zeros((500000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((500000, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,500000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 9:\n",
    "            x = np.zeros((len(s)-500000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-500000*overall-k, n_symb))\n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "\n",
    "        for i in range(len(x)//100000 -2):\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "        i = len(x)//100000 -2\n",
    "        NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                  batch_size=250,\n",
    "                  epochs=1,\n",
    "                  validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "        predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "        NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                  batch_size=250,\n",
    "                  epochs=1)\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    \n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "0\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 13s 126us/sample - loss: 1.3852 - categorical_accuracy: 0.2917 - val_loss: 1.4061 - val_categorical_accuracy: 0.2534\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3725 - categorical_accuracy: 0.3015 - val_loss: 1.3963 - val_categorical_accuracy: 0.2874\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3715 - categorical_accuracy: 0.3029 - val_loss: 1.4045 - val_categorical_accuracy: 0.2749\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 46us/sample - loss: 1.3647 - categorical_accuracy: 0.3163 - val_loss: 1.5091 - val_categorical_accuracy: 0.2707\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3594 - categorical_accuracy: 0.3253\n",
      "500000\n",
      "1\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 47us/sample - loss: 1.3606 - categorical_accuracy: 0.3237 - val_loss: 1.4576 - val_categorical_accuracy: 0.2731\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3612 - categorical_accuracy: 0.3218 - val_loss: 1.4724 - val_categorical_accuracy: 0.2819\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3578 - categorical_accuracy: 0.3258 - val_loss: 1.3643 - val_categorical_accuracy: 0.3169\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3560 - categorical_accuracy: 0.3292 - val_loss: 1.4049 - val_categorical_accuracy: 0.3024\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3575 - categorical_accuracy: 0.3277\n",
      "500000\n",
      "2\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3578 - categorical_accuracy: 0.3258 - val_loss: 1.3876 - val_categorical_accuracy: 0.3051\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3525 - categorical_accuracy: 0.3303 - val_loss: 1.3672 - val_categorical_accuracy: 0.3155\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3560 - categorical_accuracy: 0.3263 - val_loss: 1.5195 - val_categorical_accuracy: 0.2764\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 46us/sample - loss: 1.3501 - categorical_accuracy: 0.3346 - val_loss: 1.3803 - val_categorical_accuracy: 0.3092\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 31us/sample - loss: 1.3507 - categorical_accuracy: 0.3340\n",
      "500000\n",
      "3\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3528 - categorical_accuracy: 0.3318 - val_loss: 1.3739 - val_categorical_accuracy: 0.3177\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3525 - categorical_accuracy: 0.3341 - val_loss: 1.3752 - val_categorical_accuracy: 0.3192\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3494 - categorical_accuracy: 0.3352 - val_loss: 1.3650 - val_categorical_accuracy: 0.3194\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3483 - categorical_accuracy: 0.3384 - val_loss: 1.3831 - val_categorical_accuracy: 0.3181\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3460 - categorical_accuracy: 0.3393\n",
      "500000\n",
      "4\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 46us/sample - loss: 1.3483 - categorical_accuracy: 0.3371 - val_loss: 1.4835 - val_categorical_accuracy: 0.2890\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3402 - categorical_accuracy: 0.3479 - val_loss: 1.3833 - val_categorical_accuracy: 0.3223\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3399 - categorical_accuracy: 0.3485 - val_loss: 1.3487 - val_categorical_accuracy: 0.3372\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 46us/sample - loss: 1.3422 - categorical_accuracy: 0.3475 - val_loss: 1.3597 - val_categorical_accuracy: 0.3219\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3468 - categorical_accuracy: 0.3379\n",
      "500000\n",
      "5\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 46us/sample - loss: 1.3371 - categorical_accuracy: 0.3503 - val_loss: 1.3418 - val_categorical_accuracy: 0.3479\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3326 - categorical_accuracy: 0.3573 - val_loss: 1.3592 - val_categorical_accuracy: 0.3269\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3500 - categorical_accuracy: 0.3333 - val_loss: 1.4123 - val_categorical_accuracy: 0.2932\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3372 - categorical_accuracy: 0.3492 - val_loss: 1.3543 - val_categorical_accuracy: 0.3278\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3412 - categorical_accuracy: 0.3464\n",
      "500000\n",
      "6\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3357 - categorical_accuracy: 0.3511 - val_loss: 1.3485 - val_categorical_accuracy: 0.3368\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3378 - categorical_accuracy: 0.3472 - val_loss: 1.3438 - val_categorical_accuracy: 0.3411\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3327 - categorical_accuracy: 0.3523 - val_loss: 1.3433 - val_categorical_accuracy: 0.3516\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3311 - categorical_accuracy: 0.3559 - val_loss: 1.3473 - val_categorical_accuracy: 0.3357\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3416 - categorical_accuracy: 0.3394\n",
      "500000\n",
      "7\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3291 - categorical_accuracy: 0.3591 - val_loss: 1.3343 - val_categorical_accuracy: 0.3534\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 44us/sample - loss: 1.3299 - categorical_accuracy: 0.3576 - val_loss: 1.3358 - val_categorical_accuracy: 0.3490\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 4s 45us/sample - loss: 1.3333 - categorical_accuracy: 0.3522 - val_loss: 1.3384 - val_categorical_accuracy: 0.3528\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 5s 45us/sample - loss: 1.3321 - categorical_accuracy: 0.3552 - val_loss: 1.3976 - val_categorical_accuracy: 0.3225\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 3s 30us/sample - loss: 1.3331 - categorical_accuracy: 0.3513\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.49 GiB for an array with shape (500000, 20, 20) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-319a7294fe02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mpredicted_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moverall\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 64 characters context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_symb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.49 GiB for an array with shape (500000, 20, 20) and data type float64"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from plist import ProbabilityList\n",
    "\n",
    "with open('data/ecoli/Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        ecoli = inner_list\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s =  [temp_dict[i] for i in ecoli]\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "update_period = len(s)\n",
    "\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 20 #time steps\n",
    "seg_len = 20 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 256\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 32\n",
    "batch_size = 250\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "ln = True # Layer Nomalization\n",
    "\n",
    "fc = True # has fully connected layer\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "\n",
    "s = s[:-40]\n",
    "n_symb = 4\n",
    "char_list = [97, 103, 99, 116]\n",
    "\n",
    "NNPC3 = Sequential()\n",
    "NNPC3.add(LSTM(hidden_size, activation='tanh', stateful=True, batch_input_shape=(batch_size,tsteps,seg_len), return_sequences=True))\n",
    "#NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(tf.keras.layers.BatchNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,hidden_size), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# NNPC3.add(LSTM(352, activation='tanh', stateful=True, batch_input_shape=(250,tsteps,hidden_size), return_sequences=True))\n",
    "# NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(LSTM(hidden_size, activation='tanh', stateful=True, batch_input_shape=(batch_size,tsteps,hidden_size)))\n",
    "#NNPC3.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(tf.keras.layers.BatchNormalization(axis=1 , center=True , scale=True))\n",
    "NNPC3.add(Dense(n_symb))\n",
    "NNPC3.add(Activation('softmax'))\n",
    "NNPC3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "inputfile, outputfile = 'data/ecoli/Ecoli.txt', 'data/ecoli/Ecoli_NNPC_PyExact_Conf.txt'\n",
    "\n",
    "e_idx = 0\n",
    "tempdict = {}\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "\n",
    "        #np.save('temp'+str(overall),predicted_val)\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(legend[symbol])\n",
    "        h = model.get_high(legend[symbol])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(legend[symbol])\n",
    "        e_idx += 1\n",
    "        \n",
    "#     prior = [1/257 for i in range(257)]\n",
    "#     prior[256] = 1-sum(prior[:256])\n",
    "#     model = ProbabilityList(prior)   # reset model\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "        \n",
    "    \n",
    "    for overall in range(10):\n",
    "        predicted_val = []\n",
    "        if overall <= 8:\n",
    "            x = np.zeros((500000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((500000, n_symb))\n",
    "            print(len(x))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,500000*(overall+1)+k): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "#             x = np.zeros((1000000, tsteps, seg_len)) # 64 characters context\n",
    "#             y = np.zeros((1000000, n_symb))\n",
    "#             print(len(s))\n",
    "#             idx3 = 0\n",
    "#             for idx2 in range(k,1000000+k): #len(s)):\n",
    "#                 train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "#                 train_target = s[idx2]\n",
    "#                 x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "#                 y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "#                 idx3 += 1\n",
    "                \n",
    "        if overall == 9:\n",
    "            x = np.zeros((len(s)-500000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-500000*overall-k, n_symb))\n",
    "            print(len(x))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(500000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = [legend[i] for i in s[idx2-k:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0:\n",
    "            predicted_val += list(NNPC3.predict(x[0:n]))\n",
    "        if overall != 9:\n",
    "            for i in range(len(x)//100000 -2):\n",
    "                NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                      batch_size=250,\n",
    "                      epochs=1,\n",
    "                      validation_data=(x[n*(i+1):n*(i+2)], y[n*(i+1):n*(i+2)]))\n",
    "\n",
    "                predicted_val += list(NNPC3.predict(x[n*(i+1):n*(i+2)]))\n",
    "            i = len(x)//100000 -2\n",
    "            NNPC3.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)],\n",
    "                      batch_size=250,\n",
    "                      epochs=1,\n",
    "                      validation_data=(x[n*(i+1):], y[n*(i+1):]))\n",
    "            predicted_val += list(NNPC3.predict(x[n*(i+1):]))\n",
    "\n",
    "            NNPC3.fit(x[n*(i+1):], y[n*(i+1):],\n",
    "                      batch_size=250,\n",
    "                      epochs=1)\n",
    "        if overall == 9:\n",
    "            NNPC3.fit(x[0:n], y[0:n],\n",
    "                      batch_size=250,\n",
    "                      epochs=1)\n",
    "            predicted_val += list(NNPC3.predict(x[n:]))\n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        for prob_list in predicted_val:\n",
    "            for val, prob in enumerate(prob_list):\n",
    "                model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "#             model.prob_list[:4] = prob_list\n",
    "#             model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "#             model.normalize()\n",
    "#             t = int(2**16) ## New lines!\n",
    "#             l = int(model.get_low(legend[s[e_idx]])*2**16)\n",
    "#             h = int(model.get_high(legend[s[e_idx]])*2**16)\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            t = model.get_total()\n",
    "            l = model.get_low(legend[s[e_idx]])\n",
    "            h = model.get_high(legend[s[e_idx]])\n",
    "            enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "#     t = int(2**16) ## New lines!\n",
    "#     l = int(model.get_low(256)*2**16)\n",
    "#     h = int(model.get_high(256)*2**16)\n",
    "    #print(model.R)\n",
    "    #print(model.L)\n",
    "    #print(model.hb)\n",
    "    t = model.get_total()\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    e_idx += 1\n",
    "    print(e_idx)\n",
    "    enc.finish_encode(bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 06:56:09 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 431.40       Driver Version: 431.40       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 166... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8     3W /  N/A |    162MiB /  6144MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     12364    C+G   ...al\\Binaries\\Win64\\EpicGamesLauncher.exe N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
