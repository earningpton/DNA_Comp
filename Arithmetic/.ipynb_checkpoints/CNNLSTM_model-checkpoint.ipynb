{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "[97, 103, 99, 116]\n",
      "4638690\n"
     ]
    }
   ],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "list_of_lists = []\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        list_of_lists.append(inner_list)\n",
    "print(len(list_of_lists[0])) # About 4 MB\n",
    "ecoli = list_of_lists[0]\n",
    "\n",
    "\n",
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "k = 64 #context length\n",
    "n = 100000 # train every\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "#Train model\n",
    "x = np.zeros((update_period-k, k, vocab_size, 1)) # 64 characters context\n",
    "y = np.zeros((update_period-k, vocab_size))\n",
    "\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(k,len(s)):\n",
    "    train_seq = [to_categorical(legend[i], num_classes=vocab_size ) for i in s[idx2-k:idx2]] \n",
    "    train_target = to_categorical(legend[s[idx2]], num_classes=vocab_size )\n",
    "    x[idx3,:] = np.array(train_seq).reshape((k, vocab_size, 1))\n",
    "    y[idx3,:] = train_target\n",
    "    idx3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 256  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "cnn_lstm = Sequential()\n",
    "cnn_lstm.add(TimeDistributed(Conv2D(1, (2,2), activation='relu', padding='same', input_shape=(k, vocab_size,1))))\n",
    "cnn_lstm.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "cnn_lstm.add(LSTM(lstm_cells))\n",
    "cnn_lstm.add(Dense(vocab_size))\n",
    "cnn_lstm.add(Activation('softmax'))\n",
    "cnn_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "#cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//n - 1):\n",
    "    if i%5 == 0:\n",
    "        print(i)\n",
    "    cnn_lstm.fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)], batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(clf.predict_proba(x[n*(i+1):n*(i+2)]))\n",
    "predicted_onehot += list(clf.predict_proba(x[n*(len(ecoli)//n):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_onehot = []\n",
    "# for i in range(len(ecoli)//n - 1):\n",
    "#     if i%400 == 0:\n",
    "#         print(i)\n",
    "#     clf.partial_fit(x[n*i:n*(i+1)], y[n*i:n*(i+1)], np.unique(y))\n",
    "\n",
    "#     predicted_onehot += list(clf.predict_proba(x[n*(i+1):n*(i+2)]))\n",
    "# predicted_onehot += list(clf.predict_proba(x[n*(len(ecoli)//n):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
