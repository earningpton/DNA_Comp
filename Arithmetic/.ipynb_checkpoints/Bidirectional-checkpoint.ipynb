{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "from plist import ProbabilityList\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 60, 64)            9984      \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 60, 64)            120       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 64)                128       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 35,324\n",
      "Trainable params: 35,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "with open('data/ecoli/Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        ecoli = list(line)\n",
    "\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "s =  [legend[temp_dict[i]] for i in ecoli]\n",
    "\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 30 #time steps\n",
    "seg_len = 6 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 4\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 32\n",
    "batch_size = 250\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "n_symb = 4\n",
    "\n",
    "BILSTM = Sequential()\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,seg_len), return_sequences=True), input_shape=(tsteps,seg_len)))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size))))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Dense(n_symb))\n",
    "BILSTM.add(Activation('softmax'))\n",
    "BILSTM.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "BILSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)//200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4638690"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 24s 244us/sample - loss: 1.3936 - categorical_accuracy: 0.2932 - val_loss: 1.3769 - val_categorical_accuracy: 0.2954\n",
      "[[0.26773852 0.18832795 0.32454133 0.2193922 ]]\n",
      "[0.26773852 0.18832794 0.32454133 0.2193922 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 70us/sample - loss: 1.3665 - categorical_accuracy: 0.3117\n",
      "200359\n",
      "200359\n",
      "1\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 101us/sample - loss: 1.3658 - categorical_accuracy: 0.3146 - val_loss: 1.3577 - val_categorical_accuracy: 0.3195\n",
      "[[0.23507191 0.221769   0.2950498  0.24810928]]\n",
      "[0.23507193 0.22176903 0.29504976 0.2481093 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 70us/sample - loss: 1.3543 - categorical_accuracy: 0.3303\n",
      "400359\n",
      "400359\n",
      "2\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3421 - categorical_accuracy: 0.3439 - val_loss: 1.3412 - val_categorical_accuracy: 0.3432\n",
      "[[0.09538656 0.25113907 0.4520542  0.20142019]]\n",
      "[0.09538656 0.25113907 0.4520542  0.20142019]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 8s 75us/sample - loss: 1.3429 - categorical_accuracy: 0.3417\n",
      "600359\n",
      "600359\n",
      "3\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3414 - categorical_accuracy: 0.3456 - val_loss: 1.3347 - val_categorical_accuracy: 0.3496\n",
      "[[0.2021963  0.31031576 0.34023005 0.14725792]]\n",
      "[0.20219627 0.31031576 0.34023008 0.14725792]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3355 - categorical_accuracy: 0.3476\n",
      "800359\n",
      "800359\n",
      "4\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3325 - categorical_accuracy: 0.3519 - val_loss: 1.3426 - val_categorical_accuracy: 0.3445\n",
      "[[0.33313003 0.32324255 0.17678456 0.16684285]]\n",
      "[0.33313    0.32324257 0.1767846  0.16684285]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 70us/sample - loss: 1.3339 - categorical_accuracy: 0.3530\n",
      "1000359\n",
      "1000359\n",
      "5\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3390 - categorical_accuracy: 0.3462 - val_loss: 1.3352 - val_categorical_accuracy: 0.3492\n",
      "[[0.18177533 0.20659016 0.36022773 0.2514068 ]]\n",
      "[0.1817753  0.20659022 0.36022776 0.2514068 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 8s 78us/sample - loss: 1.3334 - categorical_accuracy: 0.3513\n",
      "1200359\n",
      "1200359\n",
      "6\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3410 - categorical_accuracy: 0.3433 - val_loss: 1.3401 - val_categorical_accuracy: 0.3459\n",
      "[[0.07670146 0.13963875 0.3885005  0.39515927]]\n",
      "[0.07670146 0.13963875 0.3885005  0.39515924]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3331 - categorical_accuracy: 0.3517\n",
      "1400359\n",
      "1400359\n",
      "7\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3370 - categorical_accuracy: 0.3480 - val_loss: 1.3439 - val_categorical_accuracy: 0.3384\n",
      "[[0.2583744  0.2497108  0.22719808 0.26471674]]\n",
      "[0.25837436 0.24971078 0.22719806 0.26471677]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3399 - categorical_accuracy: 0.3438\n",
      "1600359\n",
      "1600359\n",
      "8\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3401 - categorical_accuracy: 0.3475 - val_loss: 1.3345 - val_categorical_accuracy: 0.3513\n",
      "[[0.26728693 0.30517623 0.20955725 0.21797961]]\n",
      "[0.26728693 0.30517623 0.20955728 0.21797958]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 72us/sample - loss: 1.3335 - categorical_accuracy: 0.3500\n",
      "1800359\n",
      "1800359\n",
      "9\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 103us/sample - loss: 1.3326 - categorical_accuracy: 0.3537 - val_loss: 1.3299 - val_categorical_accuracy: 0.3548\n",
      "[[0.2408009  0.21735041 0.16195378 0.37989488]]\n",
      "[0.2408009  0.21735045 0.16195378 0.37989488]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3284 - categorical_accuracy: 0.3561\n",
      "2000359\n",
      "2000359\n",
      "10\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3341 - categorical_accuracy: 0.3492 - val_loss: 1.3222 - val_categorical_accuracy: 0.3638\n",
      "[[0.24479383 0.36528027 0.25023398 0.13969196]]\n",
      "[0.24479379 0.3652803  0.25023392 0.13969193]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3231 - categorical_accuracy: 0.3638\n",
      "2200359\n",
      "2200359\n",
      "11\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3226 - categorical_accuracy: 0.3658 - val_loss: 1.3214 - val_categorical_accuracy: 0.3661\n",
      "[[0.2729961  0.21061455 0.16590092 0.35048842]]\n",
      "[0.27299604 0.21061453 0.1659009  0.3504885 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3215 - categorical_accuracy: 0.3656\n",
      "2400359\n",
      "2400359\n",
      "12\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3317 - categorical_accuracy: 0.3544 - val_loss: 1.3217 - val_categorical_accuracy: 0.3645\n",
      "[[0.35151985 0.35456368 0.14302346 0.15089302]]\n",
      "[0.3515198  0.3545637  0.14302348 0.150893  ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3201 - categorical_accuracy: 0.3668\n",
      "2600359\n",
      "2600359\n",
      "13\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 105us/sample - loss: 1.3143 - categorical_accuracy: 0.3706 - val_loss: 1.3446 - val_categorical_accuracy: 0.3422\n",
      "[[0.26805633 0.3580627  0.23521376 0.13866718]]\n",
      "[0.26805636 0.3580627  0.23521376 0.13866721]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 70us/sample - loss: 1.3394 - categorical_accuracy: 0.3452\n",
      "2800359\n",
      "2800359\n",
      "14\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 104us/sample - loss: 1.3202 - categorical_accuracy: 0.3658 - val_loss: 1.3261 - val_categorical_accuracy: 0.3590\n",
      "[[0.24075004 0.2605088  0.2818405  0.21690062]]\n",
      "[0.2407501  0.26050878 0.2818405  0.2169006 ]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3260 - categorical_accuracy: 0.3595\n",
      "3000359\n",
      "3000359\n",
      "15\n",
      "Train on 100000 samples, validate on 100000 samples\n",
      "100000/100000 [==============================] - 10s 102us/sample - loss: 1.3185 - categorical_accuracy: 0.3664 - val_loss: 1.3249 - val_categorical_accuracy: 0.3601\n",
      "[[0.3023673  0.34326598 0.21756865 0.13679804]]\n",
      "[0.3023673  0.343266   0.21756867 0.13679804]\n",
      "Train on 100000 samples\n",
      "100000/100000 [==============================] - 7s 71us/sample - loss: 1.3225 - categorical_accuracy: 0.3607\n",
      "3200359\n",
      "3200359\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 549. MiB for an array with shape (200000, 60, 6) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a05d39a0fd57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mpredicted_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moverall\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 64 characters context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_symb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 549. MiB for an array with shape (200000, 60, 6) and data type float64"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "\n",
    "# from tensorflow import random\n",
    "# random.set_seed(1)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "inputfile, outputfile = 'data/ecoli/Ecoli.txt', 'data/ecoli/Ecoli.bi_complex_seed1'\n",
    "epochs = 1\n",
    "e_idx = 0\n",
    "\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    \n",
    "    \n",
    "    ## For the first n+k characters, we compress with default method\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    for symbol in s[:n+k]:\n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol)\n",
    "        h = model.get_high(symbol)\n",
    "        enc.storeRegion(l,h,t) \n",
    "        model.increment(symbol)\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [0 for i in range(257)]\n",
    "    prior[:4] = [0.25,0.25,0.25,0.25]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model, now e_idx = n+k\n",
    "    \n",
    "    for overall in range(len(s)//200000 + 1):\n",
    "        predicted_val = []\n",
    "        if overall < len(s)//200000:\n",
    "            x = np.zeros((200000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((200000, n_symb))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,200000*(overall+1)+k): #len(s)):\n",
    "                train_seq = s[idx2-k:idx2]\n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "                \n",
    "                \n",
    "        if overall == len(s)//200000:\n",
    "            x = np.zeros((len(s)-200000*overall-k, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((len(s)-200000*overall-k, n_symb))\n",
    "            print(len(x))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(200000*overall+k,len(s)): #len(s)):\n",
    "                train_seq = s[idx2-k:idx2]\n",
    "                train_target = s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "\n",
    "        if overall != 0 and overall != len(s)//200000:\n",
    "            predicted_val += list(BILSTM.predict(x[0:n]))\n",
    "        if overall != len(s)//200000:\n",
    "            BILSTM.fit(x[0:n], y[0:n],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x[n:2*n], y[n:2*n]))\n",
    "            \n",
    "            predicted_val += list(BILSTM.predict(x[n:2*n]))\n",
    "            \n",
    "            # For checking\n",
    "            x_arr = np.array(s[200000*(overall+1)-1:200000*(overall+1)+k-1]).reshape(1,tsteps,seg_len)\n",
    "            print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "            print(predicted_val[-1])\n",
    "            \n",
    "            BILSTM.fit(x[n:2*n], y[n:2*n],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs)\n",
    "            \n",
    "        if overall == len(s)//200000:\n",
    "            predicted_val += list(BILSTM.predict(x[:]))\n",
    "            \n",
    "        for prob_list in predicted_val:\n",
    "#             for val, prob in enumerate(prob_list):\n",
    "#                 model.set(val, int(prob*100000)+1)\n",
    "                \n",
    "            model.prob_list[:4] = prob_list\n",
    "            #model.prob_list[4:256] = [1/100000 for i in range(252)]\n",
    "            model.normalize()\n",
    "            t = int(100000) ## New lines!\n",
    "            l = int(model.get_low(s[e_idx])*100000)\n",
    "            h = int(model.get_high(s[e_idx])*100000)\n",
    "            enc.storeRegion(l,h,t) \n",
    "#             t = model.get_total()\n",
    "#             l = model.get_low(legend[s[e_idx]])\n",
    "#             h = model.get_high(legend[s[e_idx]])\n",
    "#             enc.storeRegion(l,h,t) \n",
    "            e_idx += 1\n",
    "        if overall != len(s)//200000: ## checking to confirm\n",
    "            print(e_idx-1) \n",
    "            print(200000*(overall+1)+k-1)\n",
    "            \n",
    "        x= None\n",
    "        y = None\n",
    "        del x\n",
    "        del y\n",
    "        predicted_val = None\n",
    "        del predicted_val\n",
    "\n",
    "    e_idx += 1\n",
    "    print(e_idx)\n",
    "    enc.finish_encode(bitout)\n",
    "    \n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(1)\n",
    "\n",
    "with open('data/ecoli/Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        ecoli = list(line)\n",
    "\n",
    "\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "char_list = [97, 103, 99, 116] # we can read this as we go\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "s =  [legend[temp_dict[i]] for i in ecoli]\n",
    "\n",
    "vocab_size = len(char_list)\n",
    "\n",
    "n = 100000 # number of samples\n",
    "tsteps = 30 #time steps\n",
    "seg_len = 6 #input_dim\n",
    "k = tsteps*seg_len # full context for each sample\n",
    "n_symb = 4\n",
    "\n",
    "# optimizer\n",
    "sgd_opt = 'adam'\n",
    "lr = 4e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.9999\n",
    "eps=1e-5\n",
    "\n",
    "# LSTM Training\n",
    "hidden_size = 32\n",
    "batch_size = 250\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "n_layer = 4 #only 4 total laters? or 4 LSTM it does say 4\n",
    "\n",
    "opt = Adam(\n",
    "    learning_rate=lr , beta_1=0.0, beta_2=beta2, epsilon=eps\n",
    ")\n",
    "\n",
    "n_symb = 4\n",
    "\n",
    "BILSTM = Sequential()\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,seg_len), return_sequences=True), input_shape=(tsteps,seg_len)))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "# BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size), return_sequences=True)))\n",
    "# BILSTM.add(BatchNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Bidirectional(LSTM(hidden_size, activation='tanh', stateful=False, batch_input_shape=(batch_size,tsteps,hidden_size))))\n",
    "BILSTM.add(LayerNormalization(axis=1 , center=True , scale=True))\n",
    "BILSTM.add(Dense(n_symb))\n",
    "BILSTM.add(Activation('softmax'))\n",
    "BILSTM.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "\n",
    "BILSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "\n",
    "# from tensorflow import random\n",
    "# random.set_seed(1)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "inputfile, outputfile = 'data/ecoli/Ecoli.bi_complex_seed1', 'data/ecoli/Ecoli_complex_decompressed.txt'\n",
    "epochs = 1\n",
    "e_idx = 0\n",
    "\n",
    "# Perform file decompression\n",
    "with open(inputfile, \"rb\") as inp, open(outputfile, \"wb\") as out:\n",
    "    bitin = arith.BitInputStream(inp)\n",
    "    \n",
    "    \n",
    "    ## For the first n+k characters, we compress with default method\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs)\n",
    "    dec = arith.ArithmeticCoder(32)\n",
    "    dec.start_decode(bitin)\n",
    "    new_s = []\n",
    "    while e_idx < n+k:\n",
    "        total = model.get_total()\n",
    "        Range = dec.R\n",
    "        offset = dec.getTarget()\n",
    "        value = dec.getTarget(total)\n",
    "        start = 0\n",
    "        end = model.get_symbol_limit()\n",
    "        while end - start > 1:\n",
    "            middle = (start + end) >> 1\n",
    "            if model.get_low(middle) > value:\n",
    "                end = middle\n",
    "            else:\n",
    "                start = middle\n",
    "        symbol = start\n",
    "        l = model.get_low(symbol) \n",
    "        h = model.get_high(symbol)\n",
    "        dec.loadRegion(l,h,total)\n",
    "        \n",
    "        model.increment(symbol)\n",
    "        out.write(bytes((char_list[symbol],)))\n",
    "        new_s.append(symbol)\n",
    "        e_idx += 1\n",
    "        \n",
    "    prior = [0 for i in range(257)]\n",
    "    prior[:4] = [0.25,0.25,0.25,0.25]\n",
    "    prior[256] = 1-sum(prior[:256])\n",
    "    model = ProbabilityList(prior)   # reset model, now e_idx = n+k\n",
    "    \n",
    "    for overall in range(len(s)//100000 + 1): # assume we save len(s), this only takes 8 bits, and cut the need for 256\n",
    "\n",
    "        if overall < len(s)//100000:\n",
    "            x = np.zeros((100000, tsteps, seg_len)) # 64 characters context\n",
    "            y = np.zeros((100000, n_symb))\n",
    "            print(overall)\n",
    "            idx3 = 0\n",
    "            for idx2 in range(100000*overall+k,100000*(overall+1)+k): #len(s)):\n",
    "                train_seq = new_s[idx2-k:idx2]\n",
    "                train_target = new_s[idx2]\n",
    "                x[idx3,:] = np.array(train_seq).reshape(tsteps,seg_len)\n",
    "                y[idx3] = to_categorical(train_target, num_classes=n_symb )\n",
    "                idx3 += 1\n",
    "            BILSTM.fit(x[0:n], y[0:n],\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs) \n",
    "        if overall == len(s)//100000:\n",
    "            segment_len = len(s)-200000*overall-k\n",
    "        else:\n",
    "            segment_len = 100000\n",
    "        print(new_s == s[:len(new_s)])  \n",
    "        temp_x = new_s[-1*k-1:-1]\n",
    "        x_arr = np.array(temp_x).reshape(1,tsteps,seg_len)\n",
    "        print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "        temp_x = new_s[-1*k:]\n",
    "        for i in range(segment_len):\n",
    "            x_arr = np.array(temp_x).reshape(1,tsteps,seg_len)\n",
    "            prob_list_temp = BILSTM(x_arr.astype(np.float32), training= False).numpy()\n",
    "            model.prob_list[:4] = prob_list_temp[0]\n",
    "\n",
    "            model.normalize()\n",
    "            \n",
    "#             print(model.prob_list[:4])\n",
    "#             print(dec.R)\n",
    "#             print(dec.getTarget(total))\n",
    "#             print(model.get_symbol_limit())\n",
    "            total = int(100000) ## New lines!\n",
    "            Range = dec.R\n",
    "            offset = dec.getTarget()\n",
    "            value = dec.getTarget(total)\n",
    "            start = 0\n",
    "            end = model.get_symbol_limit()\n",
    "            while end - start > 1:\n",
    "                middle = (start + end) >> 1\n",
    "                if int(model.get_low(middle)*100000) > value:\n",
    "                    #print(int(model.get_low(middle)*100000))\n",
    "                    end = middle\n",
    "                else:\n",
    "                    start = middle\n",
    "\n",
    "            symbol = start\n",
    "            assert symbol != 256\n",
    "            out.write(bytes((char_list[symbol],)))\n",
    "            \n",
    "            l = int(model.get_low(symbol)*100000)\n",
    "            h = int(model.get_high(symbol)*100000)\n",
    "            dec.loadRegion(l,h,total) \n",
    "\n",
    "            temp_x = temp_x[1:] + [symbol]\n",
    "            new_s.append(symbol)\n",
    "            if e_idx%20000 == 0:\n",
    "                print(e_idx)\n",
    "            e_idx += 1\n",
    "            \n",
    "        \n",
    "        print(BILSTM(x_arr.astype(np.float32), training= False).numpy())\n",
    "        print(e_idx-1) \n",
    "        print(200000*(overall+1)+k-1)\n",
    "            \n",
    "\n",
    "    e_idx += 1\n",
    "    print(e_idx)\n",
    "    \n",
    "\n",
    "toc=timeit.default_timer()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('complex.npy', new_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 2, 3, 3, 0, 0, 2, 2, 0, 0, 3, 0, 0, 0, 3, 1, 1, 2, 3, 1, 1, 2, 1, 0, 0, 1]\n",
      "[1, 2, 1, 1, 1, 2, 3, 3, 0, 0, 2, 2, 0, 0, 3, 0, 0, 0, 3, 1, 1, 2, 3, 1, 1, 2, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_s[100030:100060])\n",
    "print(s[100030:100060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 1, 0, 3, 1, 0, 3, 1, 2, 3, 1, 3, 0, 1, 1, 2, 1, 1, 3, 0, 0, 2, 1, 2, 2, 0, 3, 1]\n",
      "[3, 2, 1, 1, 0, 3, 1, 0, 3, 1, 2, 3, 1, 3, 0, 1, 1, 2, 1, 1, 3, 0, 0, 2, 1, 2, 2, 0, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(new_s[200030:200060])\n",
    "print(s[200030:200060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
