{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try Repeating Substitution Approach then 3-Gram Approach\n",
    "\n",
    "For instance, we convert ATATGATCGACGA to AT2GATCGA3\n",
    "\n",
    "Why? From EDA, each ecoli split 25%, we observe that about 10% is 2 repeat so 1/10 < $1/4 * 1/4$. About 5% is 3 repeat so 1/20 < 1/4 * 1/4 * 1/4. 4-repeat, however, is only 1.25% 1/80 < 1/256. 5-repeat is 0.5% but 6-repeat is much same as 5-repeat in ecoli but much lower in fake EColi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce81b9cc31bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mR_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# converts ATATGATCGACGA to AT2GATCGA3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ms_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def R_f(s: List[int], order: List[int] = None)-> List[int]:\n",
    "    # converts ATATGATCGACGA to AT2GATCGA3\n",
    "    s_arr = np.array(s)\n",
    "    r = [] # list\n",
    "    i = 0\n",
    "    j = 0\n",
    "    if order is None:\n",
    "        order = [2]\n",
    "    while j < len(order):\n",
    "        cur_lag = order[j]\n",
    "        checklist = np.zeros([len(s_arr)])\n",
    "        checklist[cur_lag:] = (s_arr[:-1*cur_lag]==s_arr[cur_lag:])\n",
    "        \n",
    "        \n",
    "        cur_block = s_arr[i]\n",
    "        sbe += [cur_block]\n",
    "        checklist = (np.ones([len(s_arr)])*cur_block==s_arr)\n",
    "        s_arr = s_arr[~checklist]\n",
    "        sbe += list(checklist*1)\n",
    "        i= 0\n",
    "        j += 1\n",
    "    return sbe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try 200,000 LSTM model:\n",
    "\n",
    "We want to see if it actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "seq_length = 64  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = 4  #Size of the final dense layer of the model\n",
    "lstm_cells = 16  #Size of the LSTM layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['a','t','c','g']\n",
    "legend = dict([(v, k) for k, v in enumerate(vocab)])\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# if keras\n",
    "# from keras.utils import to_categorical\n",
    "to_categorical(legend['a'],num_classes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = []\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        list_of_lists.append(inner_list)\n",
    "print(len(list_of_lists[0])) # About 4 MB\n",
    "ecoli = list_of_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "Train on 499872 samples\n",
      "Epoch 1/2\n",
      "499872/499872 [==============================] - 21s 42us/sample - loss: 1.3782 - categorical_accuracy: 0.2963\n",
      "Epoch 2/2\n",
      "499872/499872 [==============================] - 19s 38us/sample - loss: 1.3701 - categorical_accuracy: 0.3089\n",
      "[[0.3064386  0.21163759 0.2004488  0.281475  ]]\n"
     ]
    }
   ],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "update_period = 500000\n",
    "\n",
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "    s = []\n",
    "    char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "        s += [symbol[0]]\n",
    "        if symbol[0] not in char_list:\n",
    "            char_list.append(symbol[0])\n",
    "        # Train model every 200,000\n",
    "        # Read and encode one byte\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        \n",
    "        vocab_size = 0\n",
    "        if idx % update_period == 0 and idx > 0:\n",
    "            if idx == update_period or vocab_size != len(char_list): # Reinitiate model if mismatch size\n",
    "                initfreqs = fqt.FlatFrequencyTable(257)\n",
    "                model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "                \n",
    "                seq_length = 128  #Length of the sequence to be inserted into the LSTM\n",
    "                vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "                lstm_cells = 16  #Size of the LSTM layer\n",
    "\n",
    "                lstm_model = Sequential()\n",
    "                lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "                lstm_model.add(Dense(vocab_size))\n",
    "                lstm_model.add(Activation('softmax'))\n",
    "                lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "                \n",
    "                legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "            # Train model\n",
    "            x = np.zeros((update_period-128, 128, 1)) # 128 characters context\n",
    "            y = np.zeros((update_period-128, vocab_size))\n",
    "            \n",
    "            print(len(s))\n",
    "            idx3 = 0\n",
    "            for idx2 in range(idx-update_period+128,idx):\n",
    "                #print(idx2)\n",
    "                train_seq = [legend[i] for i in s[idx2-128:idx2]] \n",
    "                train_target = legend[s[idx2]]\n",
    "                x[idx3,:] = np.array(train_seq).reshape((128,1))\n",
    "                y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "                idx3 += 1\n",
    "            lstm_model.fit(x=x, y=y, batch_size=256, epochs=2, verbose=1)\n",
    "        \n",
    "        if idx >= update_period: # if more, use update period prediction instead\n",
    "#             x = np.zeros((1, 128, 1))\n",
    "#             x_pred = [legend[i] for i in s[idx-129:idx-1]]\n",
    "#             x[0,:] = np.array(train_seq).reshape((128,1))\n",
    "            \n",
    "#             predicted_onehot = lstm_model.predict(x_pred)\n",
    "            \n",
    "#             print(predicted_onehot)\n",
    "            \n",
    "            x_pred = np.zeros((1, 128, 1))\n",
    "            pred_seq = [legend[i] for i in s[-128:]]\n",
    "            x_pred[0,:] = np.array(pred_seq).reshape((128,1))\n",
    "            \n",
    "            predicted_onehot = lstm_model.predict(x_pred)\n",
    "            \n",
    "            if idx% update_period == 0:\n",
    "                print(predicted_onehot)\n",
    "            \n",
    "            for val, prob in enumerate(predicted_onehot[0]):\n",
    "#                 print(val)\n",
    "#                 print(char_list[val])\n",
    "#                 print(prob)\n",
    "#                 print(int(prob*1000))\n",
    "                model.set(char_list[val], int(prob*1000))\n",
    "                \n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < update_period: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We run out of memory. so let's try best possible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n",
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 4638562 samples\n",
      "Epoch 1/2\n",
      "4638562/4638562 [==============================] - 215s 46us/sample - loss: 1.3568 - categorical_accuracy: 0.3259\n",
      "Epoch 2/2\n",
      "4638562/4638562 [==============================] - 197s 43us/sample - loss: 1.3508 - categorical_accuracy: 0.3354\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = []\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        list_of_lists.append(inner_list)\n",
    "print(len(list_of_lists[0])) # About 4 MB\n",
    "ecoli = list_of_lists[0]\n",
    "\n",
    "\n",
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 128  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-128, 128, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-128, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(128,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-128:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((128,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "lstm_model.fit(x=x, y=y, batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "predicted_onehot = lstm_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4638562 samples\n",
      "Epoch 1/3\n",
      "4638562/4638562 [==============================] - 177s 38us/sample - loss: 1.3410 - categorical_accuracy: 0.3464\n",
      "Epoch 2/3\n",
      "4638562/4638562 [==============================] - 175s 38us/sample - loss: 1.3368 - categorical_accuracy: 0.3493\n",
      "Epoch 3/3\n",
      "4638562/4638562 [==============================] - 177s 38us/sample - loss: 1.3383 - categorical_accuracy: 0.3473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16115993c88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x=x, y=y, batch_size=256, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4638562"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_onehot = lstm_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 percent done\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 20000:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 20000:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-129]):\n",
    "                model.set(char_list[val], int(prob*100000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 20000: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\ecoli\\Fake_Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        fecoli = list(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/5\n",
      "200000/200000 [==============================] - 14s 71us/sample - loss: 1.3867 - categorical_accuracy: 0.2523\n",
      "Epoch 2/5\n",
      "200000/200000 [==============================] - 8s 38us/sample - loss: 1.3864 - categorical_accuracy: 0.2522\n",
      "Epoch 3/5\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3863 - categorical_accuracy: 0.2560\n",
      "Epoch 4/5\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3864 - categorical_accuracy: 0.2540\n",
      "Epoch 5/5\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3863 - categorical_accuracy: 0.2542\n"
     ]
    }
   ],
   "source": [
    "s = fecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 128  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-128, 128, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-128, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(128,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-128:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((128,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "lstm_model.fit(x=x[:200000], y=y[:200000], batch_size=256, epochs=5, verbose=1)\n",
    "\n",
    "predicted_onehot = lstm_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 percent done\n"
     ]
    }
   ],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(fecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(fecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200000:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 200000:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-129]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200000: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Fake_Ecoli.txt', 'data\\ecoli\\Fake_Ecoli_LSTM.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we learn from LSTM?\n",
    "\n",
    "Viable but extremely slow compared to trade-off: Better compression that GTZ (1090 kb vs GTZ 1100 kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More realistic LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4638690\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = []\n",
    "with open('data\\ecoli\\Ecoli.txt') as f:\n",
    "    for line in f:\n",
    "        #inner_list = [elt.strip() for elt in line.split()]\n",
    "        inner_list = list(line)\n",
    "        list_of_lists.append(inner_list)\n",
    "print(len(list_of_lists[0])) # About 4 MB\n",
    "ecoli = list_of_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithc as arith\n",
    "import fqt, ppm\n",
    "import contextlib, sys\n",
    "import filecmp\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 15s 74us/sample - loss: 1.3794 - categorical_accuracy: 0.2948\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 8s 38us/sample - loss: 1.3747 - categorical_accuracy: 0.3022\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3719 - categorical_accuracy: 0.3034\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3679 - categorical_accuracy: 0.3102\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 8s 38us/sample - loss: 1.3621 - categorical_accuracy: 0.3190\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3589 - categorical_accuracy: 0.3237\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3574 - categorical_accuracy: 0.3239\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 8s 39us/sample - loss: 1.3589 - categorical_accuracy: 0.3225\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      " 96512/200000 [=============>................] - ETA: 4s - loss: 1.3541 - categorical_accuracy: 0.3280"
     ]
    }
   ],
   "source": [
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 128  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-128, 128, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-128, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(128,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-128:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((128,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//200000 - 1):\n",
    "    lstm_model.fit(x=x[200000*i:200000*(i+1)], y=y[200000*i:200000*(i+1)], batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(lstm_model.predict(x[200000*(i+1):200000*(i+2)]))\n",
    "predicted_onehot += list(lstm_model.predict(x[200000*(len(ecoli)//200000):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200129:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 200129:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-200129]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200129: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM_real.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viable and if optimized will not be extremely slow compared to trade-off: Still better compression that GTZ (1098 kb vs GTZ 1100 kb). Doesn't run slowly but takes up large amount of memory. We can see clearly that given more epoch would make compression better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 20s 99us/sample - loss: 1.3809 - categorical_accuracy: 0.2885\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3758 - categorical_accuracy: 0.3006\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3709 - categorical_accuracy: 0.3032\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3707 - categorical_accuracy: 0.3045\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3665 - categorical_accuracy: 0.3127\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3638 - categorical_accuracy: 0.3176\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3590 - categorical_accuracy: 0.3246\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3561 - categorical_accuracy: 0.3271\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3530 - categorical_accuracy: 0.3307\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3597 - categorical_accuracy: 0.3224\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3547 - categorical_accuracy: 0.3278 - loss: 1.3549 \n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3492 - categorical_accuracy: 0.3361\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3636 - categorical_accuracy: 0.3194\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3567 - categorical_accuracy: 0.3271\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3529 - categorical_accuracy: 0.3308\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3549 - categorical_accuracy: 0.3285\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3528 - categorical_accuracy: 0.3316\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3518 - categorical_accuracy: 0.3320\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3582 - categorical_accuracy: 0.3236\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 14s 68us/sample - loss: 1.3612 - categorical_accuracy: 0.3191\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3568 - categorical_accuracy: 0.3249\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3538 - categorical_accuracy: 0.3290\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3537 - categorical_accuracy: 0.3283\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3575 - categorical_accuracy: 0.3230\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3601 - categorical_accuracy: 0.3228\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3559 - categorical_accuracy: 0.3297\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3506 - categorical_accuracy: 0.3357\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3467 - categorical_accuracy: 0.3407\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3482 - categorical_accuracy: 0.3381\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3532 - categorical_accuracy: 0.3330\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3494 - categorical_accuracy: 0.3374\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 63us/sample - loss: 1.3460 - categorical_accuracy: 0.3400 - loss: 1.3459  - ETA: 0s - loss: 1.3\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3455 - categorical_accuracy: 0.3410\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3395 - categorical_accuracy: 0.3504\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3354 - categorical_accuracy: 0.3549\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 70us/sample - loss: 1.3452 - categorical_accuracy: 0.3442\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3546 - categorical_accuracy: 0.3303\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3545 - categorical_accuracy: 0.3311\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3493 - categorical_accuracy: 0.3379\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 14s 70us/sample - loss: 1.3541 - categorical_accuracy: 0.3304\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3520 - categorical_accuracy: 0.3343\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 68us/sample - loss: 1.3477 - categorical_accuracy: 0.3405\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3477 - categorical_accuracy: 0.3391\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 14s 70us/sample - loss: 1.3494 - categorical_accuracy: 0.3369\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 71us/sample - loss: 1.3465 - categorical_accuracy: 0.3399\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3487 - categorical_accuracy: 0.3371\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3474 - categorical_accuracy: 0.3372\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 70us/sample - loss: 1.3433 - categorical_accuracy: 0.3434 - loss: 1.3433 - categorica\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 14s 71us/sample - loss: 1.3381 - categorical_accuracy: 0.3508\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 1.3415 - categorical_accuracy: 0.3454\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3498 - categorical_accuracy: 0.3364\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3441 - categorical_accuracy: 0.3422\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3397 - categorical_accuracy: 0.3477\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3378 - categorical_accuracy: 0.3503\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3492 - categorical_accuracy: 0.3391\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3479 - categorical_accuracy: 0.3403\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3452 - categorical_accuracy: 0.3435\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 66us/sample - loss: 1.3469 - categorical_accuracy: 0.3401\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 1.3439 - categorical_accuracy: 0.3436\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3403 - categorical_accuracy: 0.3477\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3463 - categorical_accuracy: 0.3403\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 64us/sample - loss: 1.3519 - categorical_accuracy: 0.3346\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 13s 63us/sample - loss: 1.3428 - categorical_accuracy: 0.3465 - loss: 1.3426 - ca\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3388 - categorical_accuracy: 0.3486\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 13s 67us/sample - loss: 1.3390 - categorical_accuracy: 0.3476\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 14s 68us/sample - loss: 1.3489 - categorical_accuracy: 0.3370\n"
     ]
    }
   ],
   "source": [
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 256  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-256, 256, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-256, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(256,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-256:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((256,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//200000 - 1):\n",
    "    lstm_model.fit(x=x[200000*i:200000*(i+1)], y=y[200000*i:200000*(i+1)], batch_size=256, epochs=3, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(lstm_model.predict(x[200000*(i+1):200000*(i+2)]))\n",
    "predicted_onehot += list(lstm_model.predict(x[200000*(len(ecoli)//200000):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 percent done\n"
     ]
    }
   ],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = -1\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200256:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 20256:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-200256]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200256: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM_real256.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 7s 35us/sample - loss: 1.3804 - categorical_accuracy: 0.2918\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3779 - categorical_accuracy: 0.3024\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 5s 23us/sample - loss: 1.3755 - categorical_accuracy: 0.3039\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 26us/sample - loss: 1.3745 - categorical_accuracy: 0.2990\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 5s 25us/sample - loss: 1.3717 - categorical_accuracy: 0.3046\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3702 - categorical_accuracy: 0.3077\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3664 - categorical_accuracy: 0.3137\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3649 - categorical_accuracy: 0.3158\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3637 - categorical_accuracy: 0.3178\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 27us/sample - loss: 1.3650 - categorical_accuracy: 0.3129\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 5s 23us/sample - loss: 1.3631 - categorical_accuracy: 0.3155\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3616 - categorical_accuracy: 0.3180\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3590 - categorical_accuracy: 0.3223\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3572 - categorical_accuracy: 0.3250\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 5s 23us/sample - loss: 1.3550 - categorical_accuracy: 0.3279\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 26us/sample - loss: 1.3553 - categorical_accuracy: 0.3272\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 5s 26us/sample - loss: 1.3527 - categorical_accuracy: 0.3299\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3508 - categorical_accuracy: 0.3322\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 24us/sample - loss: 1.3530 - categorical_accuracy: 0.3300\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 6s 28us/sample - loss: 1.3519 - categorical_accuracy: 0.3310\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3511 - categorical_accuracy: 0.3322\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3535 - categorical_accuracy: 0.3289\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3527 - categorical_accuracy: 0.3293\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 5s 25us/sample - loss: 1.3522 - categorical_accuracy: 0.3307\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3504 - categorical_accuracy: 0.3365\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3496 - categorical_accuracy: 0.3370\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3492 - categorical_accuracy: 0.3381\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 25us/sample - loss: 1.3457 - categorical_accuracy: 0.3419\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 6s 28us/sample - loss: 1.3449 - categorical_accuracy: 0.3425\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 5s 24us/sample - loss: 1.3444 - categorical_accuracy: 0.3435\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3447 - categorical_accuracy: 0.3412\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3439 - categorical_accuracy: 0.3415\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3435 - categorical_accuracy: 0.3418\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3385 - categorical_accuracy: 0.3514\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3377 - categorical_accuracy: 0.3527\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3371 - categorical_accuracy: 0.3524\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 24us/sample - loss: 1.3429 - categorical_accuracy: 0.3443\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3423 - categorical_accuracy: 0.3443\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3419 - categorical_accuracy: 0.3454\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3432 - categorical_accuracy: 0.3444\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3426 - categorical_accuracy: 0.3452\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3424 - categorical_accuracy: 0.3451\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3397 - categorical_accuracy: 0.3465\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3392 - categorical_accuracy: 0.3470\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3388 - categorical_accuracy: 0.3481\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3385 - categorical_accuracy: 0.3475\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3379 - categorical_accuracy: 0.3482\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3377 - categorical_accuracy: 0.3480\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3358 - categorical_accuracy: 0.3516\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 5s 26us/sample - loss: 1.3350 - categorical_accuracy: 0.3520\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3346 - categorical_accuracy: 0.3528\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 5s 23us/sample - loss: 1.3388 - categorical_accuracy: 0.3470\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3382 - categorical_accuracy: 0.3476\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3379 - categorical_accuracy: 0.3478\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3362 - categorical_accuracy: 0.3519\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3355 - categorical_accuracy: 0.3526\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 20us/sample - loss: 1.3349 - categorical_accuracy: 0.3525\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 1.3364 - categorical_accuracy: 0.3517\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3360 - categorical_accuracy: 0.3521\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 22us/sample - loss: 1.3357 - categorical_accuracy: 0.3509\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3380 - categorical_accuracy: 0.3482\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3373 - categorical_accuracy: 0.3492\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3368 - categorical_accuracy: 0.3501\n",
      "Train on 200000 samples\n",
      "Epoch 1/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3335 - categorical_accuracy: 0.3532\n",
      "Epoch 2/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3328 - categorical_accuracy: 0.3529\n",
      "Epoch 3/3\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.3325 - categorical_accuracy: 0.3535\n"
     ]
    }
   ],
   "source": [
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 32  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 16  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-32, 32, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-32, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(256,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-32:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((32,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//200000 - 1):\n",
    "    lstm_model.fit(x=x[200000*i:200000*(i+1)], y=y[200000*i:200000*(i+1)], batch_size=256, epochs=3, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(lstm_model.predict(x[200000*(i+1):200000*(i+2)]))\n",
    "predicted_onehot += list(lstm_model.predict(x[200000*(len(ecoli)//200000):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 percent done\n"
     ]
    }
   ],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200033:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 200033:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-200033]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200033: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM_real32.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 10s 50us/sample - loss: 1.3801 - categorical_accuracy: 0.2917\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3762 - categorical_accuracy: 0.3010\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 32us/sample - loss: 1.3740 - categorical_accuracy: 0.3001\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3685 - categorical_accuracy: 0.3092\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3641 - categorical_accuracy: 0.3153\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3568 - categorical_accuracy: 0.3257\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3534 - categorical_accuracy: 0.3290\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 29us/sample - loss: 1.3499 - categorical_accuracy: 0.3343\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3462 - categorical_accuracy: 0.3393\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 5s 27us/sample - loss: 1.3430 - categorical_accuracy: 0.3435\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3450 - categorical_accuracy: 0.3414\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3428 - categorical_accuracy: 0.3438\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3436 - categorical_accuracy: 0.3434\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 29us/sample - loss: 1.3423 - categorical_accuracy: 0.3440\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3458 - categorical_accuracy: 0.3387\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3448 - categorical_accuracy: 0.3404\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3426 - categorical_accuracy: 0.3433\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3419 - categorical_accuracy: 0.3449\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3370 - categorical_accuracy: 0.3509\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3358 - categorical_accuracy: 0.3503\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3362 - categorical_accuracy: 0.3492\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3352 - categorical_accuracy: 0.3519\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3298 - categorical_accuracy: 0.3601\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3283 - categorical_accuracy: 0.3599\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3329 - categorical_accuracy: 0.3533\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3317 - categorical_accuracy: 0.3546\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3339 - categorical_accuracy: 0.3524\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 28us/sample - loss: 1.3331 - categorical_accuracy: 0.3541\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3300 - categorical_accuracy: 0.3558\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3291 - categorical_accuracy: 0.3574\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3284 - categorical_accuracy: 0.3560\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3274 - categorical_accuracy: 0.3575\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3244 - categorical_accuracy: 0.3608\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3235 - categorical_accuracy: 0.3617\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3285 - categorical_accuracy: 0.3569\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3275 - categorical_accuracy: 0.3579\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3248 - categorical_accuracy: 0.3632\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3234 - categorical_accuracy: 0.3635\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3264 - categorical_accuracy: 0.3579\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3253 - categorical_accuracy: 0.3593\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3269 - categorical_accuracy: 0.3583\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 29us/sample - loss: 1.3258 - categorical_accuracy: 0.3599\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 6s 31us/sample - loss: 1.3223 - categorical_accuracy: 0.3643\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 6s 30us/sample - loss: 1.3214 - categorical_accuracy: 0.3633\n"
     ]
    }
   ],
   "source": [
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 64  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 32  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-64, 64, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-64, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(256,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-64:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((64,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//200000 - 1):\n",
    "    lstm_model.fit(x=x[200000*i:200000*(i+1)], y=y[200000*i:200000*(i+1)], batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(lstm_model.predict(x[200000*(i+1):200000*(i+2)]))\n",
    "predicted_onehot += list(lstm_model.predict(x[200000*(len(ecoli)//200000):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 percent done\n"
     ]
    }
   ],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200065:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 200065:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-200065]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200065: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM_real64.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 103, 99, 116]\n",
      "4638690\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 16s 82us/sample - loss: 1.3796 - categorical_accuracy: 0.2938\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3758 - categorical_accuracy: 0.3008\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 10s 49us/sample - loss: 1.3749 - categorical_accuracy: 0.2972\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3726 - categorical_accuracy: 0.3015\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3687 - categorical_accuracy: 0.3092\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3672 - categorical_accuracy: 0.3118\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3673 - categorical_accuracy: 0.3097\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3654 - categorical_accuracy: 0.3134\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3625 - categorical_accuracy: 0.3193\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3596 - categorical_accuracy: 0.3236\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3594 - categorical_accuracy: 0.3222\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3596 - categorical_accuracy: 0.3237\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3596 - categorical_accuracy: 0.3218\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3515 - categorical_accuracy: 0.3324\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 10s 48us/sample - loss: 1.3658 - categorical_accuracy: 0.3139\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3753 - categorical_accuracy: 0.3005\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3709 - categorical_accuracy: 0.3095\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3685 - categorical_accuracy: 0.3127\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3665 - categorical_accuracy: 0.3151\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3650 - categorical_accuracy: 0.3170\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3644 - categorical_accuracy: 0.3158\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - ETA: 0s - loss: 1.3634 - categorical_accuracy: 0.31 - 9s 46us/sample - loss: 1.3634 - categorical_accuracy: 0.3177\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3594 - categorical_accuracy: 0.3258\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3579 - categorical_accuracy: 0.3281\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3602 - categorical_accuracy: 0.3223\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3606 - categorical_accuracy: 0.3222\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3582 - categorical_accuracy: 0.3262\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3500 - categorical_accuracy: 0.3378\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3457 - categorical_accuracy: 0.3424\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3499 - categorical_accuracy: 0.3375\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3509 - categorical_accuracy: 0.3351\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3529 - categorical_accuracy: 0.3329\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3519 - categorical_accuracy: 0.3337\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3451 - categorical_accuracy: 0.3426\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3553 - categorical_accuracy: 0.3291\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3430 - categorical_accuracy: 0.3445\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3398 - categorical_accuracy: 0.3486\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3472 - categorical_accuracy: 0.3405\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 45us/sample - loss: 1.3572 - categorical_accuracy: 0.3303\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3596 - categorical_accuracy: 0.3242\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 47us/sample - loss: 1.3566 - categorical_accuracy: 0.3284\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3465 - categorical_accuracy: 0.3407\n",
      "Train on 200000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3372 - categorical_accuracy: 0.3511\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 9s 46us/sample - loss: 1.3371 - categorical_accuracy: 0.3513\n"
     ]
    }
   ],
   "source": [
    "s = ecoli\n",
    "char_list = [97, 103, 99, 116]\n",
    "print(char_list)\n",
    "update_period = len(s)\n",
    "\n",
    "seq_length = 160  #Length of the sequence to be inserted into the LSTM\n",
    "vocab_size = len(char_list)  #Size of the final dense layer of the model\n",
    "lstm_cells = 24  #Size of the LSTM layer\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(lstm_cells, input_shape=(seq_length, 1)))\n",
    "lstm_model.add(Dense(vocab_size))\n",
    "lstm_model.add(Activation('softmax'))\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "legend = dict([(v, k) for k, v in enumerate(char_list)]) # map character to 0,1,2,3,4, etc.\n",
    "\n",
    "temp_dict = {'a':97,'g': 103,'c': 99,'t': 116}\n",
    "s = [temp_dict[i] for i in s]\n",
    "# Train model\n",
    "x = np.zeros((update_period-160, 160, 1)) # 128 characters context\n",
    "y = np.zeros((update_period-160, vocab_size))\n",
    "\n",
    "print(len(s))\n",
    "idx3 = 0\n",
    "for idx2 in range(256,len(s)):\n",
    "    #print(idx2)\n",
    "    train_seq = [legend[i] for i in s[idx2-160:idx2]] \n",
    "    train_target = legend[s[idx2]]\n",
    "    x[idx3,:] = np.array(train_seq).reshape((160,1))\n",
    "    y[idx3,:] = to_categorical(train_target, num_classes=vocab_size )\n",
    "    idx3 += 1\n",
    "predicted_onehot = []\n",
    "for i in range(len(ecoli)//200000 - 1):\n",
    "    lstm_model.fit(x=x[200000*i:200000*(i+1)], y=y[200000*i:200000*(i+1)], batch_size=256, epochs=2, verbose=1)\n",
    "\n",
    "    predicted_onehot += list(lstm_model.predict(x[200000*(i+1):200000*(i+2)]))\n",
    "predicted_onehot += list(lstm_model.predict(x[200000*(len(ecoli)//200000):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 percent done\n"
     ]
    }
   ],
   "source": [
    "def LSTMcompress(inp, bitout):\n",
    "    initfreqs = fqt.FlatFrequencyTable(257)\n",
    "    model = fqt.SimpleFrequencyTable(initfreqs) # For the first 200,000\n",
    "    enc = arith.ArithmeticCoder(32)\n",
    "    enc.start_encode(bitout) # New line!\n",
    "\n",
    "#     char_list = [] ## Need list of char\n",
    "    idx = 0\n",
    "    while True:\n",
    "        symbol = inp.read(1)\n",
    "        if len(symbol) == 0:\n",
    "                break\n",
    "#         if symbol[0] not in char_list:\n",
    "#             char_list.append(symbol[0])\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "        ## Progress Evaluation ## only internal\n",
    "        if idx % (len(ecoli)//10) == 0:\n",
    "            print(str(10*idx//(len(ecoli)//10)) + ' percent done')\n",
    "            clear_output(wait = True)\n",
    "        if idx == 200161:\n",
    "            initfreqs = fqt.FlatFrequencyTable(257)\n",
    "            model = fqt.SimpleFrequencyTable(initfreqs) # reset the model\n",
    "        if idx >= 200161:\n",
    "            for val, prob in enumerate(predicted_onehot[idx-200161]):\n",
    "                model.set(char_list[val], int(prob*1000000))\n",
    "            \n",
    "        t = model.get_total() ## New lines!\n",
    "        l = model.get_low(symbol[0])\n",
    "        h = model.get_high(symbol[0])\n",
    "        enc.storeRegion(l,h,t) \n",
    "        \n",
    "        if idx < 200161: # back up before LSTM model\n",
    "            model.increment(symbol[0])\n",
    "    t = model.get_total() ## New lines!\n",
    "    l = model.get_low(256)\n",
    "    h = model.get_high(256)\n",
    "    enc.storeRegion(l,h,t)\n",
    "    enc.finish_encode(bitout)  # New line!\n",
    "inputfile, outputfile = 'data\\ecoli\\Ecoli.txt', 'data\\ecoli\\Ecoli_LSTM_real160.txt'\n",
    "\n",
    "#Perform file compression\n",
    "with open(inputfile, \"rb\") as inp, \\\n",
    "        contextlib.closing(arith.BitOutputStream(open(outputfile, \"wb\"))) as bitout:\n",
    "    LSTMcompress(inp, bitout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
